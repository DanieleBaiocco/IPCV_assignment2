{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# Assignment Module 2: Product Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTQUJ4uYH1w"
      },
      "source": [
        "## Preliminaries: the dataset\n",
        "\n",
        "The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "The products belong to the following 43 classes:\n",
        "```\n",
        "0.  Apple\n",
        "1.  Avocado\n",
        "2.  Banana\n",
        "3.  Kiwi\n",
        "4.  Lemon\n",
        "5.  Lime\n",
        "6.  Mango\n",
        "7.  Melon\n",
        "8.  Nectarine\n",
        "9.  Orange\n",
        "10. Papaya\n",
        "11. Passion-Fruit\n",
        "12. Peach\n",
        "13. Pear\n",
        "14. Pineapple\n",
        "15. Plum\n",
        "16. Pomegranate\n",
        "17. Red-Grapefruit\n",
        "18. Satsumas\n",
        "19. Juice\n",
        "20. Milk\n",
        "21. Oatghurt\n",
        "22. Oat-Milk\n",
        "23. Sour-Cream\n",
        "24. Sour-Milk\n",
        "25. Soyghurt\n",
        "26. Soy-Milk\n",
        "27. Yoghurt\n",
        "28. Asparagus\n",
        "29. Aubergine\n",
        "30. Cabbage\n",
        "31. Carrots\n",
        "32. Cucumber\n",
        "33. Garlic\n",
        "34. Ginger\n",
        "35. Leek\n",
        "36. Mushroom\n",
        "37. Onion\n",
        "38. Pepper\n",
        "39. Potato\n",
        "40. Red-Beet\n",
        "41. Tomato\n",
        "42. Zucchini\n",
        "```\n",
        "\n",
        "The dataset is split into training (`train`), validation (`val`), and test (`test`) set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pdrmJRnJPd8"
      },
      "source": [
        "The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "POMX_3x-_bZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfbfbd3-d385-4b20-91b9-73862cb2788b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroceryStoreDataset'...\n",
            "remote: Enumerating objects: 6559, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293\u001b[K\n",
            "Receiving objects: 100% (6559/6559), 116.26 MiB | 24.53 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n",
            "Updating files: 100% (5717/5717), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hiF8xGEYlsu8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Tuple, Callable, Optional\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jROSO2qVDxdD"
      },
      "outputs": [],
      "source": [
        "class GroceryStoreDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split: str, transform: Callable[[PIL.Image.Image], Tensor]) -> None:\n",
        "        super().__init__()\n",
        "        self.root = Path(\"GroceryStoreDataset/dataset\")\n",
        "        self.split = split\n",
        "        self.paths, self.labels = self.read_file()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx])\n",
        "        label = self.labels[idx]\n",
        "        tensor_img = self.transform(img)\n",
        "        return tensor_img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                # path, fine-grained class, coarse-grained class\n",
        "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
        "                paths.append(path)\n",
        "                labels.append(int(label))\n",
        "        return paths, labels\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBch3dpwNSsW"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n",
        "\n",
        "- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PENSO CHE LA COSA MIGLIORE DA FARE SIA INIZIARE CON UN MODELLO MOLTO COMPLICATO CHE OVERFITTI SUL TRAINING DATA. A QUEL PUNTO INSERIRE DELLE REGULARIZATION TECNIQUES CHE MI FACCIANO ARRIVARE A UNA EFFECTIVE CAPACITY OTTIMALE."
      ],
      "metadata": {
        "id": "3jMo0SqDq1EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mypy\n",
        "# Simple mypy cell magic for Colab\n",
        "from IPython.core.magic import register_cell_magic\n",
        "from IPython import get_ipython\n",
        "from mypy import api\n",
        "\n",
        "@register_cell_magic\n",
        "def mypy(line, cell):\n",
        "    for output in api.run(['-c', '\\n' + cell] + line.split()):\n",
        "        if output and not output.startswith('Success'):\n",
        "            raise TypeError(output)\n",
        "    get_ipython().run_cell(cell)"
      ],
      "metadata": {
        "id": "zmBr-o8PuLSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6377bfc-574d-4263-bc9b-9114f1940d5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mypy\n",
            "  Downloading mypy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy) (4.12.2)\n",
            "Collecting mypy-extensions>=1.0.0 (from mypy)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy) (2.0.1)\n",
            "Installing collected packages: mypy-extensions, mypy\n",
            "Successfully installed mypy-1.10.1 mypy-extensions-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecnf = {'transform': transforms.PILToTensor(),\n",
        "                      'batch_size': 32,\n",
        "                      'num_workers': 2}"
      ],
      "metadata": {
        "id": "hRrsLDEYomDC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%mypy --ignore-missing-imports\n",
        "def create_dataloader(split: str,\n",
        "                      transform: Callable[[PIL.Image.Image],Tensor],\n",
        "                      dataset_creation: Callable[[str, Callable[[PIL.Image.Image],Tensor]], Dataset],\n",
        "                      batch_size: int,\n",
        "                      shuffle: bool,\n",
        "                      num_workers: int) -> DataLoader:\n",
        "      dataset = dataset_creation(split, transform)\n",
        "      return DataLoader(dataset, batch_size = batch_size, shuffle = shuffle, num_workers = num_workers)\n",
        "\n",
        "train_dataloader = create_dataloader('train', basecnf['transform'], GroceryStoreDataset, basecnf['batch_size'], True, basecnf['num_workers'])\n",
        "val_dataloader = create_dataloader('val', basecnf['transform'], GroceryStoreDataset, basecnf['batch_size'], False, basecnf['num_workers'])\n",
        "test_dataloader = create_dataloader('test', basecnf['transform'], GroceryStoreDataset, basecnf['batch_size'], False, basecnf['num_workers'])"
      ],
      "metadata": {
        "id": "w6KdSkr_jqgQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "res = GroceryStoreDataset('train', basecnf['transform'])\n",
        "res.__getitem__(3)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmnCvWZs3Jko",
        "outputId": "f6207407-9c04-429c-ebaa-a7d0ada51254"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 348, 348])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GroceryStoreModel(torch.nn.Module):\n",
        "    def __init__(self, nonlin_fn: Callable[[], torch.nn.Module],\n",
        "                 nor_fn: Callable[[int], torch.nn.Module],\n",
        "                 dropout_head,\n",
        "                 nor_head_fn,\n",
        "                 dropout_fe,\n",
        "                 stem_convs_fn: Callable[[int, Callable[[], torch.nn.Module], Callable[[int], torch.nn.Module]], torch.nn.Sequential],\n",
        "                 stem_output_channels: int,\n",
        "                 num_stages: int,\n",
        "                 stage_fn: Callable[[int, Callable[[], torch.nn.Module], Callable[[int], torch.nn.Module]], torch.nn.Sequential],\n",
        "                 head_fn: Callable[[int, int], torch.nn.Sequential],\n",
        "                 num_classes: int):\n",
        "        super(GroceryStoreModel, self).__init__()\n",
        "        self.stem = torch.nn.Sequential(\n",
        "            stem_convs_fn(stem_output_channels, nonlin_fn, nor_fn, dropout_head),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        self.stages = torch.nn.ModuleList()\n",
        "        in_channels = stem_output_channels\n",
        "        for _ in range(num_stages):\n",
        "            stage = stage_fn(in_channels, nonlin_fn, nor_fn,dropout_head)\n",
        "            self.stages.append(stage)\n",
        "            in_channels *= 2\n",
        "        self.head = head_fn(in_channels, num_classes, nonlin_fn, nor_head_fn, dropout_fe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.stem(x)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wGSSm1u5TE7l"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vorrei usare sia globalavgpooling che fc layers staccati uno sull'altro.\n",
        "# vorrei fare anche un paragone col numero di parametri risultanti dalla rete e salvare tutto in una tabella.\n",
        "# tipo comparando il risultato delle performance tra modello con fclayers e modello con global avg poooling INSIEME anche a  una comparazione sul numero di parametri\n",
        "# usati.\n",
        "#from thop import profile\n",
        "#flops, params = profile(model, inputs=(inputs,))\n",
        "\n",
        "def block(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int,\n",
        "          normalization_layer: torch.nn.Module, activation_layer: torch.nn.Module, dropout_rate) -> torch.nn.Sequential:\n",
        "    layers = [\n",
        "        torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "        normalization_layer,\n",
        "        activation_layer\n",
        "    ]\n",
        "    if dropout_rate > 0:\n",
        "        layers.append(torch.nn.Dropout(dropout_rate))\n",
        "    return torch.nn.Sequential(*layers)\n",
        "\n",
        "def stem_convs1(out_channels: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate:float = 0.0) -> torch.nn.Sequential:\n",
        "    return torch.nn.Sequential(\n",
        "        block(3, out_channels, 3, 2, 1, normalization_fn(out_channels), activation_fn(),dropout_rate),\n",
        "        block(out_channels, out_channels, 3, 1, 1, normalization_fn(out_channels), activation_fn(),dropout_rate),\n",
        "        block(out_channels, out_channels, 3, 1, 1, normalization_fn(out_channels), activation_fn(),dropout_rate)\n",
        "    )\n",
        "\n",
        "\n",
        "def stem_convs2(out_channels: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate:float = 0.0) -> torch.nn.Sequential:\n",
        "    return block(3, out_channels, 7, 2, 1, normalization_fn(out_channels), activation_fn(),dropout_rate)\n",
        "\n",
        "\n",
        "def stage_fn1(in_channels: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate:float = 0.0) -> torch.nn.Sequential:\n",
        "    out_channels = in_channels * 2\n",
        "    return torch.nn.Sequential(\n",
        "        block(in_channels, out_channels, 3, 1, 1, normalization_fn(out_channels), activation_fn(),dropout_rate),\n",
        "        block(out_channels, out_channels, 3, 1, 1, normalization_fn(out_channels), activation_fn() ,dropout_rate),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "def stage_fn2(in_channels: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate:float = 0.0) -> torch.nn.Sequential:\n",
        "    out_channels = in_channels * 2\n",
        "    return torch.nn.Sequential(\n",
        "        block(in_channels, out_channels, 5, 1, 2, normalization_fn(out_channels), activation_fn(),dropout_rate),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "def head_fn1(in_channels: int, num_classes: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate) -> torch.nn.Sequential:\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Dropout(dropout_rate),\n",
        "        torch.nn.Linear(in_channels, num_classes)\n",
        "    )\n",
        "\n",
        "def head_fn2(in_channels: int, num_classes: int, activation_fn: Callable[[], torch.nn.Module], normalization_fn: Callable[[int], torch.nn.Module], dropout_rate):\n",
        "      return torch.nn.Sequential(\n",
        "        torch.nn.Flatten(),  # Flatten the input tensor to feed into fully connected layers\n",
        "        torch.nn.Linear(in_channels, 256),  # First fully connected layer\n",
        "        normalization_fn(256),\n",
        "        # Batch normalization layer\n",
        "        activation_fn(),\n",
        "        torch.nn.Dropout(dropout_rate),# Activation layer without in-place modification\n",
        "        torch.nn.Linear(256, 128),  # Second fully connected layer\n",
        "        normalization_fn(128),  # Batch normalization layer\n",
        "        activation_fn(),\n",
        "        torch.nn.Dropout(dropout_rate),# Activation layer without in-place modification\n",
        "        torch.nn.Linear(128, num_classes)  # Final classification layer\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "model = GroceryStoreModel(\n",
        "    nonlin_fn=lambda: torch.nn.LeakyReLU(),\n",
        "    nor_fn=lambda channels: torch.nn.BatchNorm2d(channels),\n",
        "    nor_head_fn = lambda channels: torch.nn.BatchNorm1d(channels),\n",
        "    dropout_head = 0.0,\n",
        "    dropout_fe = 0.0,\n",
        "    stem_convs_fn=stem_convs1,\n",
        "    stem_output_channels=64,\n",
        "    num_stages=3,\n",
        "    stage_fn=stage_fn1,\n",
        "    head_fn=head_fn1,\n",
        "    num_classes=42\n",
        ")"
      ],
      "metadata": {
        "id": "IJKkAbDxpaNm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(3, 348, 348))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1QeJAyzCD9u",
        "outputId": "73c16fac-387c-480a-b978-2c94bff5dff2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 174, 174]           1,792\n",
            "       BatchNorm2d-2         [-1, 64, 174, 174]             128\n",
            "         LeakyReLU-3         [-1, 64, 174, 174]               0\n",
            "            Conv2d-4         [-1, 64, 174, 174]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 174, 174]             128\n",
            "         LeakyReLU-6         [-1, 64, 174, 174]               0\n",
            "            Conv2d-7         [-1, 64, 174, 174]          36,928\n",
            "       BatchNorm2d-8         [-1, 64, 174, 174]             128\n",
            "         LeakyReLU-9         [-1, 64, 174, 174]               0\n",
            "        MaxPool2d-10           [-1, 64, 87, 87]               0\n",
            "           Conv2d-11          [-1, 128, 87, 87]          73,856\n",
            "      BatchNorm2d-12          [-1, 128, 87, 87]             256\n",
            "        LeakyReLU-13          [-1, 128, 87, 87]               0\n",
            "           Conv2d-14          [-1, 128, 87, 87]         147,584\n",
            "      BatchNorm2d-15          [-1, 128, 87, 87]             256\n",
            "        LeakyReLU-16          [-1, 128, 87, 87]               0\n",
            "        MaxPool2d-17          [-1, 128, 43, 43]               0\n",
            "           Conv2d-18          [-1, 256, 43, 43]         295,168\n",
            "      BatchNorm2d-19          [-1, 256, 43, 43]             512\n",
            "        LeakyReLU-20          [-1, 256, 43, 43]               0\n",
            "           Conv2d-21          [-1, 256, 43, 43]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 43, 43]             512\n",
            "        LeakyReLU-23          [-1, 256, 43, 43]               0\n",
            "        MaxPool2d-24          [-1, 256, 21, 21]               0\n",
            "           Conv2d-25          [-1, 512, 21, 21]       1,180,160\n",
            "      BatchNorm2d-26          [-1, 512, 21, 21]           1,024\n",
            "        LeakyReLU-27          [-1, 512, 21, 21]               0\n",
            "           Conv2d-28          [-1, 512, 21, 21]       2,359,808\n",
            "      BatchNorm2d-29          [-1, 512, 21, 21]           1,024\n",
            "        LeakyReLU-30          [-1, 512, 21, 21]               0\n",
            "        MaxPool2d-31          [-1, 512, 10, 10]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
            "          Flatten-33                  [-1, 512]               0\n",
            "          Dropout-34                  [-1, 512]               0\n",
            "           Linear-35                   [-1, 42]          21,546\n",
            "================================================================\n",
            "Total params: 4,747,818\n",
            "Trainable params: 4,747,818\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.39\n",
            "Forward/backward pass size (MB): 216.17\n",
            "Params size (MB): 18.11\n",
            "Estimated Total Size (MB): 235.67\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "res = GroceryStoreDataset('train', basecnf['transform'])\n",
        "res.__getitem__(3)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6207407-9c04-429c-ebaa-a7d0ada51254",
        "id": "QLFdEa5eCeYm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 348, 348])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "res = GroceryStoreDataset('train', basecnf['transform'])\n",
        "res.__getitem__(3)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6207407-9c04-429c-ebaa-a7d0ada51254",
        "id": "-NSSbtqBCdgW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 348, 348])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkWEqSPoUIL3"
      },
      "source": [
        "## Part 2: fine-tune an existing network\n",
        "\n",
        "Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
        "\n",
        "1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n",
        "1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}