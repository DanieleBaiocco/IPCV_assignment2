Training and evaluating base model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.66 M  
fwd MACs:                                                               134.959 GMACs
fwd FLOPs:                                                              271.577 GFLOPS
fwd+bwd MACs:                                                           404.876 GMACs
fwd+bwd FLOPs:                                                          814.732 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.66 M = 100% Params, 134.96 GMACs = 100% MACs, 271.58 GFLOPS = 100% FLOPs
  (stem): Sequential(
    76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.12 GFLOPS = 58.5892% FLOPs
    (0): Sequential(
      76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.05 GFLOPS = 58.5645% FLOPs
      (0): ConvBlock(
        1.92 K = 0.0412% Params, 1.81 GMACs = 1.3426% MACs, 3.89 GFLOPS = 1.4332% FLOPs
        (conv2d): Conv2d(1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.69 GFLOPS = 1.3591% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 1.1631% Params, 13.96 GMACs = 10.3429% MACs, 28.34 GFLOPS = 10.4341% FLOPs
      (0): InceptionBlock(
        24.03 K = 0.5156% Params, 6.17 GMACs = 4.5747% MACs, 12.53 GFLOPS = 4.6147% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.1239% Params, 1.48 GMACs = 1.094% MACs, 3 GFLOPS = 1.1058% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 0.2997% Params, 3.62 GMACs = 2.6852% MACs, 7.3 GFLOPS = 2.6873% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.12 GFLOPS = 0.4139% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 0.6475% Params, 7.78 GMACs = 5.7682% MACs, 15.77 GFLOPS = 5.807% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.1459% Params, 1.74 GMACs = 1.2929% MACs, 3.54 GFLOPS = 1.3035% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 0.3217% Params, 3.89 GMACs = 2.8841% MACs, 7.83 GFLOPS = 2.885% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.21 GFLOPS = 0.8155% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 4.6113% Params, 13.96 GMACs = 10.3429% MACs, 28.13 GFLOPS = 10.3569% FLOPs
      (0): InceptionBlock(
        95.17 K = 2.042% Params, 6.17 GMACs = 4.5747% MACs, 12.44 GFLOPS = 4.5808% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 0.4896% Params, 1.48 GMACs = 1.094% MACs, 2.98 GFLOPS = 1.0965% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 1.1926% Params, 3.62 GMACs = 2.6852% MACs, 7.27 GFLOPS = 2.678% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.1 GFLOPS = 0.4046% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 2.5693% Params, 7.78 GMACs = 5.7682% MACs, 15.67 GFLOPS = 5.77% FLOPs
        (branch1): ConvBlock(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 0.5774% Params, 1.74 GMACs = 1.2929% MACs, 3.51 GFLOPS = 1.2942% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 1.2805% Params, 3.89 GMACs = 2.8841% MACs, 7.81 GFLOPS = 2.8757% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 18.3627% Params, 13.96 GMACs = 10.3429% MACs, 28.02 GFLOPS = 10.3183% FLOPs
      (0): InceptionBlock(
        378.75 K = 8.1267% Params, 6.17 GMACs = 4.5747% MACs, 12.39 GFLOPS = 4.5638% FLOPs
        (branch1): ConvBlock(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
          (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 1.9458% Params, 1.48 GMACs = 1.094% MACs, 2.97 GFLOPS = 1.0919% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 4.7582% Params, 3.62 GMACs = 2.6852% MACs, 7.26 GFLOPS = 2.6734% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 10.236% Params, 7.78 GMACs = 5.7682% MACs, 15.62 GFLOPS = 5.7514% FLOPs
        (branch1): ConvBlock(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 2.2974% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2896% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 5.1097% Params, 3.89 GMACs = 2.8841% MACs, 7.8 GFLOPS = 2.8711% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.42 M = 73.2859% Params, 13.96 GMACs = 10.3429% MACs, 27.97 GFLOPS = 10.299% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.4244% Params, 6.17 GMACs = 4.5747% MACs, 12.37 GFLOPS = 4.5553% FLOPs
        (branch1): ConvBlock(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          361.6 K = 7.7587% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0896% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.89 K = 19.0081% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.6711% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3977% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 40.8615% Params, 7.78 GMACs = 5.7682% MACs, 15.59 GFLOPS = 5.7422% FLOPs
        (branch1): ConvBlock(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
          (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          427.14 K = 9.1649% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2873% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          951.42 K = 20.4143% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8688% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
            (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 6.68 MFLOPS = 0.0025% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 5.64 MFLOPS = 0.0021% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:18<00:00,  2.29it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]
Epoch 1/30
Train Loss: 3.1121 Train Accuracy: 0.1947
Val Loss: 3.1028 Val Accuracy: 0.1250
Epoch 2/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 2/30
Train Loss: 2.2145 Train Accuracy: 0.3792
Val Loss: 2.2452 Val Accuracy: 0.3412
Epoch 3/30: 100%|██████████| 42/42 [00:18<00:00,  2.26it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s]
Epoch 3/30
Train Loss: 1.7645 Train Accuracy: 0.4455
Val Loss: 2.1493 Val Accuracy: 0.3412
Epoch 4/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  3.26it/s]
Epoch 4/30
Train Loss: 1.5029 Train Accuracy: 0.5303
Val Loss: 2.0811 Val Accuracy: 0.3953
Epoch 5/30: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
Epoch 5/30
Train Loss: 1.4445 Train Accuracy: 0.5413
Val Loss: 2.4074 Val Accuracy: 0.3243
Epoch 6/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 6/30
Train Loss: 1.3200 Train Accuracy: 0.5818
Val Loss: 2.0667 Val Accuracy: 0.3581
Epoch 7/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 7/30
Train Loss: 1.2447 Train Accuracy: 0.5890
Val Loss: 2.4509 Val Accuracy: 0.3750
Epoch 8/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]
Epoch 8/30
Train Loss: 1.2220 Train Accuracy: 0.6125
Val Loss: 2.5603 Val Accuracy: 0.3311
Epoch 9/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 9/30
Train Loss: 1.1116 Train Accuracy: 0.6371
Val Loss: 2.1564 Val Accuracy: 0.3851
Epoch 10/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  3.01it/s]
Epoch 10/30
Train Loss: 0.9314 Train Accuracy: 0.7023
Val Loss: 2.4021 Val Accuracy: 0.3716
Epoch 11/30: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 11/30
Train Loss: 0.9026 Train Accuracy: 0.7061
Val Loss: 2.1518 Val Accuracy: 0.4155
Epoch 12/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  2.94it/s]
Epoch 12/30
Train Loss: 0.8000 Train Accuracy: 0.7417
Val Loss: 1.8527 Val Accuracy: 0.4831
Epoch 13/30: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]
Epoch 13/30
Train Loss: 0.7025 Train Accuracy: 0.7807
Val Loss: 2.3946 Val Accuracy: 0.4595
Epoch 14/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 14/30
Train Loss: 0.7158 Train Accuracy: 0.7712
Val Loss: 1.5635 Val Accuracy: 0.5304
Epoch 15/30: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.30it/s]
Epoch 15/30
Train Loss: 0.6545 Train Accuracy: 0.7989
Val Loss: 2.0471 Val Accuracy: 0.4797
Epoch 16/30: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:01<00:00,  2.91it/s]
Epoch 16/30
Train Loss: 0.5777 Train Accuracy: 0.8193
Val Loss: 1.5158 Val Accuracy: 0.5574
Epoch 17/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 17/30
Train Loss: 0.5826 Train Accuracy: 0.8265
Val Loss: 1.7169 Val Accuracy: 0.5169
Epoch 18/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s]
Epoch 18/30
Train Loss: 0.5299 Train Accuracy: 0.8303
Val Loss: 1.4551 Val Accuracy: 0.6047
Epoch 19/30: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]
Epoch 19/30
Train Loss: 0.4088 Train Accuracy: 0.8686
Val Loss: 1.3856 Val Accuracy: 0.5845
Epoch 20/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 20/30
Train Loss: 0.4038 Train Accuracy: 0.8841
Val Loss: 1.4338 Val Accuracy: 0.5980
Epoch 21/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]
Epoch 21/30
Train Loss: 0.3371 Train Accuracy: 0.8981
Val Loss: 1.3885 Val Accuracy: 0.6351
Epoch 22/30: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:01<00:00,  2.95it/s]
Epoch 22/30
Train Loss: 0.3424 Train Accuracy: 0.9068
Val Loss: 1.1921 Val Accuracy: 0.6824
Epoch 23/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  3.20it/s]
Epoch 23/30
Train Loss: 0.3093 Train Accuracy: 0.9102
Val Loss: 1.3238 Val Accuracy: 0.6216
Epoch 24/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.00it/s]
Epoch 24/30
Train Loss: 0.2834 Train Accuracy: 0.9280
Val Loss: 1.2332 Val Accuracy: 0.6655
Epoch 25/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.34it/s]
Epoch 25/30
Train Loss: 0.2663 Train Accuracy: 0.9292
Val Loss: 1.1869 Val Accuracy: 0.6588
Epoch 26/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.02it/s]
Epoch 26/30
Train Loss: 0.2175 Train Accuracy: 0.9394
Val Loss: 1.3520 Val Accuracy: 0.6284
Epoch 27/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
Epoch 27/30
Train Loss: 0.2179 Train Accuracy: 0.9386
Val Loss: 1.2678 Val Accuracy: 0.6520
Epoch 28/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s]
Epoch 28/30
Train Loss: 0.1967 Train Accuracy: 0.9477
Val Loss: 1.1828 Val Accuracy: 0.6520
Epoch 29/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]
Epoch 29/30
Train Loss: 0.1831 Train Accuracy: 0.9527
Val Loss: 1.2024 Val Accuracy: 0.6554
Epoch 30/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  2.90it/s]
Epoch 30/30
Train Loss: 0.1908 Train Accuracy: 0.9455
Val Loss: 1.2631 Val Accuracy: 0.6318
Training and evaluating 3stages model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  1.22 M  
fwd MACs:                                                               120.999 GMACs
fwd FLOPs:                                                              243.606 GFLOPS
fwd+bwd MACs:                                                           362.996 GMACs
fwd+bwd FLOPs:                                                          730.818 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  1.22 M = 100% Params, 121 GMACs = 100% MACs, 243.61 GFLOPS = 100% FLOPs
  (stem): Sequential(
    76.03 K = 6.2167% Params, 79.12 GMACs = 65.3903% MACs, 159.12 GFLOPS = 65.3166% FLOPs
    (0): Sequential(
      76.03 K = 6.2167% Params, 79.12 GMACs = 65.3903% MACs, 159.05 GFLOPS = 65.2891% FLOPs
      (0): ConvBlock(
        1.92 K = 0.157% Params, 1.81 GMACs = 1.4975% MACs, 3.89 GFLOPS = 1.5978% FLOPs
        (conv2d): Conv2d(1.79 K = 0.1465% Params, 1.81 GMACs = 1.4975% MACs, 3.69 GFLOPS = 1.5151% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0551% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0275% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        37.06 K = 3.0299% Params, 38.65 GMACs = 31.9464% MACs, 77.58 GFLOPS = 31.8456% FLOPs
        (conv2d): Conv2d(36.93 K = 3.0194% Params, 38.65 GMACs = 31.9464% MACs, 77.38 GFLOPS = 31.763% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0551% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0275% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        37.06 K = 3.0299% Params, 38.65 GMACs = 31.9464% MACs, 77.58 GFLOPS = 31.8456% FLOPs
        (conv2d): Conv2d(36.93 K = 3.0194% Params, 38.65 GMACs = 31.9464% MACs, 77.38 GFLOPS = 31.763% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0551% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0275% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0275% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 4.4323% Params, 13.96 GMACs = 11.5362% MACs, 28.34 GFLOPS = 11.6322% FLOPs
      (0): InceptionBlock(
        24.03 K = 1.965% Params, 6.17 GMACs = 5.1025% MACs, 12.53 GFLOPS = 5.1446% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.1753% Params, 536.87 MMACs = 0.4437% MACs, 1.11 GFLOPS = 0.4545% FLOPs
          (conv2d): Conv2d(2.08 K = 0.1701% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.4723% Params, 1.48 GMACs = 1.2202% MACs, 3 GFLOPS = 1.2328% FLOPs
          (0): ConvBlock(
            1.07 K = 0.0877% Params, 268.44 MMACs = 0.2218% MACs, 553.65 MFLOPS = 0.2273% FLOPs
            (conv2d): Conv2d(1.04 K = 0.085% Params, 268.44 MMACs = 0.2218% MACs, 541.07 MFLOPS = 0.2221% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0026% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.3846% Params, 1.21 GMACs = 0.9983% MACs, 2.45 GFLOPS = 1.0055% FLOPs
            (conv2d): Conv2d(4.64 K = 0.3794% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9952% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 1.1421% Params, 3.62 GMACs = 2.995% MACs, 7.3 GFLOPS = 2.9959% FLOPs
          (0): ConvBlock(
            1.07 K = 0.0877% Params, 268.44 MMACs = 0.2218% MACs, 553.65 MFLOPS = 0.2273% FLOPs
            (conv2d): Conv2d(1.04 K = 0.085% Params, 268.44 MMACs = 0.2218% MACs, 541.07 MFLOPS = 0.2221% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0026% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 1.0544% Params, 3.36 GMACs = 2.7731% MACs, 6.74 GFLOPS = 2.7686% FLOPs
            (conv2d): Conv2d(12.83 K = 1.0492% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7583% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.1753% Params, 536.87 MMACs = 0.4437% MACs, 1.12 GFLOPS = 0.4614% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.1753% Params, 536.87 MMACs = 0.4437% MACs, 1.11 GFLOPS = 0.4545% FLOPs
            (conv2d): Conv2d(2.08 K = 0.1701% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 2.4673% Params, 7.78 GMACs = 6.4336% MACs, 15.77 GFLOPS = 6.4738% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.3428% Params, 1.07 GMACs = 0.8874% MACs, 2.18 GFLOPS = 0.8953% FLOPs
          (conv2d): Conv2d(4.13 K = 0.3375% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.885% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.556% Params, 1.74 GMACs = 1.442% MACs, 3.54 GFLOPS = 1.4532% FLOPs
          (0): ConvBlock(
            2.1 K = 0.1714% Params, 536.87 MMACs = 0.4437% MACs, 1.09 GFLOPS = 0.4477% FLOPs
            (conv2d): Conv2d(2.06 K = 0.1688% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0026% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.3846% Params, 1.21 GMACs = 0.9983% MACs, 2.45 GFLOPS = 1.0055% FLOPs
            (conv2d): Conv2d(4.64 K = 0.3794% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9952% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 1.2258% Params, 3.89 GMACs = 3.2168% MACs, 7.83 GFLOPS = 3.2162% FLOPs
          (0): ConvBlock(
            2.1 K = 0.1714% Params, 536.87 MMACs = 0.4437% MACs, 1.09 GFLOPS = 0.4477% FLOPs
            (conv2d): Conv2d(2.06 K = 0.1688% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0026% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 1.0544% Params, 3.36 GMACs = 2.7731% MACs, 6.74 GFLOPS = 2.7686% FLOPs
            (conv2d): Conv2d(12.83 K = 1.0492% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7583% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.3428% Params, 1.07 GMACs = 0.8874% MACs, 2.21 GFLOPS = 0.9091% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0138% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.3428% Params, 1.07 GMACs = 0.8874% MACs, 2.18 GFLOPS = 0.8953% FLOPs
            (conv2d): Conv2d(4.13 K = 0.3375% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.885% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0138% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 17.5723% Params, 13.96 GMACs = 11.5362% MACs, 28.13 GFLOPS = 11.5461% FLOPs
      (0): InceptionBlock(
        95.17 K = 7.7814% Params, 6.17 GMACs = 5.1025% MACs, 12.44 GFLOPS = 5.1067% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.6855% Params, 536.87 MMACs = 0.4437% MACs, 1.09 GFLOPS = 0.4477% FLOPs
          (conv2d): Conv2d(8.26 K = 0.6751% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 1.8655% Params, 1.48 GMACs = 1.2202% MACs, 2.98 GFLOPS = 1.2224% FLOPs
          (0): ConvBlock(
            4.19 K = 0.3428% Params, 268.44 MMACs = 0.2218% MACs, 545.26 MFLOPS = 0.2238% FLOPs
            (conv2d): Conv2d(4.13 K = 0.3375% Params, 268.44 MMACs = 0.2218% MACs, 538.97 MFLOPS = 0.2212% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 1.5228% Params, 1.21 GMACs = 0.9983% MACs, 2.43 GFLOPS = 0.9986% FLOPs
            (conv2d): Conv2d(18.5 K = 1.5123% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9935% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 4.5448% Params, 3.62 GMACs = 2.995% MACs, 7.27 GFLOPS = 2.9855% FLOPs
          (0): ConvBlock(
            4.19 K = 0.3428% Params, 268.44 MMACs = 0.2218% MACs, 545.26 MFLOPS = 0.2238% FLOPs
            (conv2d): Conv2d(4.13 K = 0.3375% Params, 268.44 MMACs = 0.2218% MACs, 538.97 MFLOPS = 0.2212% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 4.2021% Params, 3.36 GMACs = 2.7731% MACs, 6.73 GFLOPS = 2.7617% FLOPs
            (conv2d): Conv2d(51.26 K = 4.1916% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7565% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.6855% Params, 536.87 MMACs = 0.4437% MACs, 1.1 GFLOPS = 0.4511% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.6855% Params, 536.87 MMACs = 0.4437% MACs, 1.09 GFLOPS = 0.4477% FLOPs
            (conv2d): Conv2d(8.26 K = 0.6751% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 9.7909% Params, 7.78 GMACs = 6.4336% MACs, 15.67 GFLOPS = 6.4325% FLOPs
        (branch1): ConvBlock(
          16.58 K = 1.3553% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.8884% FLOPs
          (conv2d): Conv2d(16.45 K = 1.3449% Params, 1.07 GMACs = 0.8874% MACs, 2.15 GFLOPS = 0.8833% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 2.2005% Params, 1.74 GMACs = 1.442% MACs, 3.51 GFLOPS = 1.4428% FLOPs
          (0): ConvBlock(
            8.29 K = 0.6777% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs
            (conv2d): Conv2d(8.22 K = 0.6724% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4416% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 1.5228% Params, 1.21 GMACs = 0.9983% MACs, 2.43 GFLOPS = 0.9986% FLOPs
            (conv2d): Conv2d(18.5 K = 1.5123% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9935% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 4.8797% Params, 3.89 GMACs = 3.2168% MACs, 7.81 GFLOPS = 3.2059% FLOPs
          (0): ConvBlock(
            8.29 K = 0.6777% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs
            (conv2d): Conv2d(8.22 K = 0.6724% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4416% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0052% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 4.2021% Params, 3.36 GMACs = 2.7731% MACs, 6.73 GFLOPS = 2.7617% FLOPs
            (conv2d): Conv2d(51.26 K = 4.1916% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7565% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 1.3553% Params, 1.07 GMACs = 0.8874% MACs, 2.18 GFLOPS = 0.8953% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 1.3553% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.8884% FLOPs
            (conv2d): Conv2d(16.45 K = 1.3449% Params, 1.07 GMACs = 0.8874% MACs, 2.15 GFLOPS = 0.8833% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0069% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 69.975% Params, 13.96 GMACs = 11.5362% MACs, 28.02 GFLOPS = 11.5031% FLOPs
      (0): InceptionBlock(
        378.75 K = 30.9686% Params, 6.17 GMACs = 5.1025% MACs, 12.39 GFLOPS = 5.0878% FLOPs
        (branch1): ConvBlock(
          33.15 K = 2.7107% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs
          (conv2d): Conv2d(32.9 K = 2.6897% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4416% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 7.4151% Params, 1.48 GMACs = 1.2202% MACs, 2.97 GFLOPS = 1.2173% FLOPs
          (0): ConvBlock(
            16.58 K = 1.3553% Params, 268.44 MMACs = 0.2218% MACs, 541.07 MFLOPS = 0.2221% FLOPs
            (conv2d): Conv2d(16.45 K = 1.3449% Params, 268.44 MMACs = 0.2218% MACs, 537.92 MFLOPS = 0.2208% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 6.0598% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9952% FLOPs
            (conv2d): Conv2d(73.86 K = 6.0388% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9926% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 18.1322% Params, 3.62 GMACs = 2.995% MACs, 7.26 GFLOPS = 2.9804% FLOPs
          (0): ConvBlock(
            16.58 K = 1.3553% Params, 268.44 MMACs = 0.2218% MACs, 541.07 MFLOPS = 0.2221% FLOPs
            (conv2d): Conv2d(16.45 K = 1.3449% Params, 268.44 MMACs = 0.2218% MACs, 537.92 MFLOPS = 0.2208% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 16.7768% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7583% FLOPs
            (conv2d): Conv2d(204.93 K = 16.7559% Params, 3.36 GMACs = 2.7731% MACs, 6.71 GFLOPS = 2.7557% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 2.7107% Params, 536.87 MMACs = 0.4437% MACs, 1.09 GFLOPS = 0.4459% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 2.7107% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4442% FLOPs
            (conv2d): Conv2d(32.9 K = 2.6897% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4416% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 39.0064% Params, 7.78 GMACs = 6.4336% MACs, 15.62 GFLOPS = 6.4118% FLOPs
        (branch1): ConvBlock(
          65.92 K = 5.3899% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.885% FLOPs
          (conv2d): Conv2d(65.66 K = 5.369% Params, 1.07 GMACs = 0.8874% MACs, 2.15 GFLOPS = 0.8824% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 8.7547% Params, 1.74 GMACs = 1.442% MACs, 3.5 GFLOPS = 1.4377% FLOPs
          (0): ConvBlock(
            32.96 K = 2.695% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs
            (conv2d): Conv2d(32.83 K = 2.6845% Params, 536.87 MMACs = 0.4437% MACs, 1.07 GFLOPS = 0.4412% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 6.0598% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9952% FLOPs
            (conv2d): Conv2d(73.86 K = 6.0388% Params, 1.21 GMACs = 0.9983% MACs, 2.42 GFLOPS = 0.9926% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 19.4718% Params, 3.89 GMACs = 3.2168% MACs, 7.8 GFLOPS = 3.2007% FLOPs
          (0): ConvBlock(
            32.96 K = 2.695% Params, 536.87 MMACs = 0.4437% MACs, 1.08 GFLOPS = 0.4425% FLOPs
            (conv2d): Conv2d(32.83 K = 2.6845% Params, 536.87 MMACs = 0.4437% MACs, 1.07 GFLOPS = 0.4412% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0105% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 16.7768% Params, 3.36 GMACs = 2.7731% MACs, 6.72 GFLOPS = 2.7583% FLOPs
            (conv2d): Conv2d(204.93 K = 16.7559% Params, 3.36 GMACs = 2.7731% MACs, 6.71 GFLOPS = 2.7557% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 5.3899% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.8884% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 5.3899% Params, 1.07 GMACs = 0.8874% MACs, 2.16 GFLOPS = 0.885% FLOPs
            (conv2d): Conv2d(65.66 K = 5.369% Params, 1.07 GMACs = 0.8874% MACs, 2.15 GFLOPS = 0.8824% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0209% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0017% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0034% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    22.06 K = 1.8037% Params, 1.41 MMACs = 0.0012% MACs, 4.92 MFLOPS = 0.002% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0009% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(22.06 K = 1.8037% Params, 1.41 MMACs = 0.0012% MACs, 2.82 MFLOPS = 0.0012% FLOPs, in_features=512, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.23it/s]
Epoch 1/30
Train Loss: 3.2667 Train Accuracy: 0.1879
Val Loss: 3.1009 Val Accuracy: 0.1926
Epoch 2/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]
Epoch 2/30
Train Loss: 2.5281 Train Accuracy: 0.3117
Val Loss: 2.6254 Val Accuracy: 0.2973
Epoch 3/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]
Epoch 3/30
Train Loss: 2.0937 Train Accuracy: 0.4167
Val Loss: 2.2642 Val Accuracy: 0.3311
Epoch 4/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  3.01it/s]
Epoch 4/30
Train Loss: 1.7862 Train Accuracy: 0.4754
Val Loss: 2.6518 Val Accuracy: 0.3007
Epoch 5/30: 100%|██████████| 42/42 [00:18<00:00,  2.27it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  3.23it/s]
Epoch 5/30
Train Loss: 1.6152 Train Accuracy: 0.4981
Val Loss: 2.0335 Val Accuracy: 0.4088
Epoch 6/30: 100%|██████████| 42/42 [00:17<00:00,  2.33it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  3.20it/s]
Epoch 6/30
Train Loss: 1.4528 Train Accuracy: 0.5417
Val Loss: 1.8462 Val Accuracy: 0.4662
Epoch 7/30: 100%|██████████| 42/42 [00:18<00:00,  2.31it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 7/30
Train Loss: 1.3340 Train Accuracy: 0.5841
Val Loss: 1.8497 Val Accuracy: 0.4155
Epoch 8/30: 100%|██████████| 42/42 [00:17<00:00,  2.35it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 8/30
Train Loss: 1.2944 Train Accuracy: 0.5996
Val Loss: 1.8475 Val Accuracy: 0.4527
Epoch 9/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:02<00:00,  2.27it/s]
Epoch 9/30
Train Loss: 1.1063 Train Accuracy: 0.6534
Val Loss: 1.7993 Val Accuracy: 0.4662
Epoch 10/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]
Epoch 10/30
Train Loss: 0.9004 Train Accuracy: 0.7258
Val Loss: 1.7808 Val Accuracy: 0.4797
Epoch 11/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:02<00:00,  2.33it/s]
Epoch 11/30
Train Loss: 0.8529 Train Accuracy: 0.7383
Val Loss: 1.5532 Val Accuracy: 0.5068
Epoch 12/30: 100%|██████████| 42/42 [00:18<00:00,  2.26it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  3.43it/s]
Epoch 12/30
Train Loss: 0.7970 Train Accuracy: 0.7568
Val Loss: 1.4851 Val Accuracy: 0.5608
Epoch 13/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.30it/s]
Epoch 13/30
Train Loss: 0.7208 Train Accuracy: 0.7852
Val Loss: 1.4952 Val Accuracy: 0.6047
Epoch 14/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  2.95it/s]
Epoch 14/30
Train Loss: 0.6985 Train Accuracy: 0.7792
Val Loss: 1.5430 Val Accuracy: 0.5777
Epoch 15/30: 100%|██████████| 42/42 [00:17<00:00,  2.37it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.29it/s]
Epoch 15/30
Train Loss: 0.6119 Train Accuracy: 0.8197
Val Loss: 1.5055 Val Accuracy: 0.5912
Epoch 16/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:02<00:00,  2.23it/s]
Epoch 16/30
Train Loss: 0.5739 Train Accuracy: 0.8254
Val Loss: 1.5037 Val Accuracy: 0.6081
Epoch 17/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 17/30
Train Loss: 0.5538 Train Accuracy: 0.8386
Val Loss: 1.3895 Val Accuracy: 0.6250
Epoch 18/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  2.78it/s]
Epoch 18/30
Train Loss: 0.5227 Train Accuracy: 0.8455
Val Loss: 1.5778 Val Accuracy: 0.5338
Epoch 19/30: 100%|██████████| 42/42 [00:18<00:00,  2.30it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 19/30
Train Loss: 0.4863 Train Accuracy: 0.8583
Val Loss: 1.1600 Val Accuracy: 0.6351
Epoch 20/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 20/30
Train Loss: 0.4170 Train Accuracy: 0.8818
Val Loss: 1.3509 Val Accuracy: 0.5946
Epoch 21/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  2.99it/s]
Epoch 21/30
Train Loss: 0.4155 Train Accuracy: 0.8739
Val Loss: 1.1213 Val Accuracy: 0.6824
Epoch 22/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 22/30
Train Loss: 0.3614 Train Accuracy: 0.9015
Val Loss: 1.2011 Val Accuracy: 0.6554
Epoch 23/30: 100%|██████████| 42/42 [00:17<00:00,  2.35it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:02<00:00,  2.30it/s]
Epoch 23/30
Train Loss: 0.3387 Train Accuracy: 0.9000
Val Loss: 1.0908 Val Accuracy: 0.6926
Epoch 24/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
Epoch 24/30
Train Loss: 0.2980 Train Accuracy: 0.9205
Val Loss: 1.1059 Val Accuracy: 0.6858
Epoch 25/30: 100%|██████████| 42/42 [00:17<00:00,  2.35it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  2.59it/s]
Epoch 25/30
Train Loss: 0.2615 Train Accuracy: 0.9352
Val Loss: 1.1097 Val Accuracy: 0.6791
Epoch 26/30: 100%|██████████| 42/42 [00:18<00:00,  2.27it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 26/30
Train Loss: 0.2435 Train Accuracy: 0.9398
Val Loss: 1.0520 Val Accuracy: 0.6926
Epoch 27/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 27/30
Train Loss: 0.2338 Train Accuracy: 0.9402
Val Loss: 1.0465 Val Accuracy: 0.7027
Epoch 28/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]
Epoch 28/30
Train Loss: 0.2270 Train Accuracy: 0.9447
Val Loss: 1.1273 Val Accuracy: 0.6588
Epoch 29/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  3.20it/s]
Epoch 29/30
Train Loss: 0.2346 Train Accuracy: 0.9390
Val Loss: 1.0703 Val Accuracy: 0.6757
Epoch 30/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]
Epoch 30/30
Train Loss: 0.2319 Train Accuracy: 0.9470
Val Loss: 1.0466 Val Accuracy: 0.6689
Training and evaluating no_drphead model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.66 M  
fwd MACs:                                                               134.959 GMACs
fwd FLOPs:                                                              271.577 GFLOPS
fwd+bwd MACs:                                                           404.876 GMACs
fwd+bwd FLOPs:                                                          814.732 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.66 M = 100% Params, 134.96 GMACs = 100% MACs, 271.58 GFLOPS = 100% FLOPs
  (stem): Sequential(
    76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.12 GFLOPS = 58.5892% FLOPs
    (0): Sequential(
      76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.05 GFLOPS = 58.5645% FLOPs
      (0): ConvBlock(
        1.92 K = 0.0412% Params, 1.81 GMACs = 1.3426% MACs, 3.89 GFLOPS = 1.4332% FLOPs
        (conv2d): Conv2d(1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.69 GFLOPS = 1.3591% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 1.1631% Params, 13.96 GMACs = 10.3429% MACs, 28.34 GFLOPS = 10.4341% FLOPs
      (0): InceptionBlock(
        24.03 K = 0.5156% Params, 6.17 GMACs = 4.5747% MACs, 12.53 GFLOPS = 4.6147% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.1239% Params, 1.48 GMACs = 1.094% MACs, 3 GFLOPS = 1.1058% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 0.2997% Params, 3.62 GMACs = 2.6852% MACs, 7.3 GFLOPS = 2.6873% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.12 GFLOPS = 0.4139% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 0.6475% Params, 7.78 GMACs = 5.7682% MACs, 15.77 GFLOPS = 5.807% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.1459% Params, 1.74 GMACs = 1.2929% MACs, 3.54 GFLOPS = 1.3035% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 0.3217% Params, 3.89 GMACs = 2.8841% MACs, 7.83 GFLOPS = 2.885% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.21 GFLOPS = 0.8155% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 4.6113% Params, 13.96 GMACs = 10.3429% MACs, 28.13 GFLOPS = 10.3569% FLOPs
      (0): InceptionBlock(
        95.17 K = 2.042% Params, 6.17 GMACs = 4.5747% MACs, 12.44 GFLOPS = 4.5808% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 0.4896% Params, 1.48 GMACs = 1.094% MACs, 2.98 GFLOPS = 1.0965% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 1.1926% Params, 3.62 GMACs = 2.6852% MACs, 7.27 GFLOPS = 2.678% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.1 GFLOPS = 0.4046% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 2.5693% Params, 7.78 GMACs = 5.7682% MACs, 15.67 GFLOPS = 5.77% FLOPs
        (branch1): ConvBlock(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 0.5774% Params, 1.74 GMACs = 1.2929% MACs, 3.51 GFLOPS = 1.2942% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 1.2805% Params, 3.89 GMACs = 2.8841% MACs, 7.81 GFLOPS = 2.8757% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 18.3627% Params, 13.96 GMACs = 10.3429% MACs, 28.02 GFLOPS = 10.3183% FLOPs
      (0): InceptionBlock(
        378.75 K = 8.1267% Params, 6.17 GMACs = 4.5747% MACs, 12.39 GFLOPS = 4.5638% FLOPs
        (branch1): ConvBlock(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
          (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 1.9458% Params, 1.48 GMACs = 1.094% MACs, 2.97 GFLOPS = 1.0919% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 4.7582% Params, 3.62 GMACs = 2.6852% MACs, 7.26 GFLOPS = 2.6734% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 10.236% Params, 7.78 GMACs = 5.7682% MACs, 15.62 GFLOPS = 5.7514% FLOPs
        (branch1): ConvBlock(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 2.2974% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2896% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 5.1097% Params, 3.89 GMACs = 2.8841% MACs, 7.8 GFLOPS = 2.8711% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.42 M = 73.2859% Params, 13.96 GMACs = 10.3429% MACs, 27.97 GFLOPS = 10.299% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.4244% Params, 6.17 GMACs = 4.5747% MACs, 12.37 GFLOPS = 4.5553% FLOPs
        (branch1): ConvBlock(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          361.6 K = 7.7587% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0896% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.89 K = 19.0081% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.6711% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3977% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 40.8615% Params, 7.78 GMACs = 5.7682% MACs, 15.59 GFLOPS = 5.7422% FLOPs
        (branch1): ConvBlock(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
          (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          427.14 K = 9.1649% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2873% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          951.42 K = 20.4143% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8688% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
            (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 6.68 MFLOPS = 0.0025% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
    (3): Linear(44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 5.64 MFLOPS = 0.0021% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]
Epoch 1/30
Train Loss: 3.0994 Train Accuracy: 0.1841
Val Loss: 3.1133 Val Accuracy: 0.1453
Epoch 2/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:02<00:00,  2.31it/s]
Epoch 2/30
Train Loss: 2.2415 Train Accuracy: 0.3686
Val Loss: 2.4941 Val Accuracy: 0.3277
Epoch 3/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 3/30
Train Loss: 1.7291 Train Accuracy: 0.4674
Val Loss: 2.3670 Val Accuracy: 0.3514
Epoch 4/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]
Epoch 4/30
Train Loss: 1.4563 Train Accuracy: 0.5284
Val Loss: 2.2584 Val Accuracy: 0.3209
Epoch 5/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
Epoch 5/30
Train Loss: 1.3722 Train Accuracy: 0.5712
Val Loss: 2.1531 Val Accuracy: 0.3750
Epoch 6/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  2.87it/s]
Epoch 6/30
Train Loss: 1.2628 Train Accuracy: 0.6057
Val Loss: 2.7050 Val Accuracy: 0.3142
Epoch 7/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 7/30
Train Loss: 1.1935 Train Accuracy: 0.6170
Val Loss: 3.8075 Val Accuracy: 0.2128
Epoch 8/30: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s]
Epoch 8/30
Train Loss: 1.1100 Train Accuracy: 0.6432
Val Loss: 1.5398 Val Accuracy: 0.5169
Epoch 9/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 9/30
Train Loss: 0.9601 Train Accuracy: 0.6909
Val Loss: 2.5485 Val Accuracy: 0.3649
Epoch 10/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  2.99it/s]
Epoch 10/30
Train Loss: 0.9137 Train Accuracy: 0.7163
Val Loss: 2.2805 Val Accuracy: 0.3716
Epoch 11/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 11/30
Train Loss: 0.8172 Train Accuracy: 0.7417
Val Loss: 1.6665 Val Accuracy: 0.4932
Epoch 12/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]
Epoch 12/30
Train Loss: 0.6996 Train Accuracy: 0.7727
Val Loss: 1.5663 Val Accuracy: 0.5541
Epoch 13/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 13/30
Train Loss: 0.6740 Train Accuracy: 0.7962
Val Loss: 2.1043 Val Accuracy: 0.4628
Epoch 14/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]
Epoch 14/30
Train Loss: 0.6735 Train Accuracy: 0.7845
Val Loss: 1.7916 Val Accuracy: 0.5101
Epoch 15/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]
Epoch 15/30
Train Loss: 0.5407 Train Accuracy: 0.8223
Val Loss: 1.9785 Val Accuracy: 0.5000
Epoch 16/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:01<00:00,  2.82it/s]
Epoch 16/30
Train Loss: 0.4546 Train Accuracy: 0.8614
Val Loss: 1.4465 Val Accuracy: 0.6250
Epoch 17/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.01it/s]
Epoch 17/30
Train Loss: 0.4560 Train Accuracy: 0.8561
Val Loss: 1.6295 Val Accuracy: 0.5541
Epoch 18/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  2.92it/s]
Epoch 18/30
Train Loss: 0.4411 Train Accuracy: 0.8693
Val Loss: 1.4280 Val Accuracy: 0.5980
Epoch 19/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]
Epoch 19/30
Train Loss: 0.4223 Train Accuracy: 0.8780
Val Loss: 1.5209 Val Accuracy: 0.6149
Epoch 20/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  2.88it/s]
Epoch 20/30
Train Loss: 0.3748 Train Accuracy: 0.8909
Val Loss: 1.6354 Val Accuracy: 0.5946
Epoch 21/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 21/30
Train Loss: 0.3074 Train Accuracy: 0.9106
Val Loss: 1.2532 Val Accuracy: 0.6351
Epoch 22/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:01<00:00,  2.90it/s]
Epoch 22/30
Train Loss: 0.2634 Train Accuracy: 0.9155
Val Loss: 1.4479 Val Accuracy: 0.5642
Epoch 23/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 23/30
Train Loss: 0.2655 Train Accuracy: 0.9212
Val Loss: 1.2027 Val Accuracy: 0.6520
Epoch 24/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]
Epoch 24/30
Train Loss: 0.2286 Train Accuracy: 0.9311
Val Loss: 1.2488 Val Accuracy: 0.6284
Epoch 25/30: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.23it/s]
Epoch 25/30
Train Loss: 0.1828 Train Accuracy: 0.9485
Val Loss: 1.2261 Val Accuracy: 0.6486
Epoch 26/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  2.90it/s]
Epoch 26/30
Train Loss: 0.1981 Train Accuracy: 0.9492
Val Loss: 1.2666 Val Accuracy: 0.6655
Epoch 27/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.29it/s]
Epoch 27/30
Train Loss: 0.1677 Train Accuracy: 0.9557
Val Loss: 1.3005 Val Accuracy: 0.6318
Epoch 28/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s]
Epoch 28/30
Train Loss: 0.1711 Train Accuracy: 0.9572
Val Loss: 1.3402 Val Accuracy: 0.6182
Epoch 29/30: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  3.30it/s]
Epoch 29/30
Train Loss: 0.1581 Train Accuracy: 0.9591
Val Loss: 1.2497 Val Accuracy: 0.6453
Epoch 30/30: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  2.95it/s]
Epoch 30/30
Train Loss: 0.1530 Train Accuracy: 0.9591
Val Loss: 1.3170 Val Accuracy: 0.6351
Training and evaluating no_drpfe model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.66 M  
fwd MACs:                                                               134.959 GMACs
fwd FLOPs:                                                              271.577 GFLOPS
fwd+bwd MACs:                                                           404.876 GMACs
fwd+bwd FLOPs:                                                          814.732 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.66 M = 100% Params, 134.96 GMACs = 100% MACs, 271.58 GFLOPS = 100% FLOPs
  (stem): Sequential(
    76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.12 GFLOPS = 58.5892% FLOPs
    (0): Sequential(
      76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.05 GFLOPS = 58.5645% FLOPs
      (0): ConvBlock(
        1.92 K = 0.0412% Params, 1.81 GMACs = 1.3426% MACs, 3.89 GFLOPS = 1.4332% FLOPs
        (conv2d): Conv2d(1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.69 GFLOPS = 1.3591% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 1.1631% Params, 13.96 GMACs = 10.3429% MACs, 28.34 GFLOPS = 10.4341% FLOPs
      (0): InceptionBlock(
        24.03 K = 0.5156% Params, 6.17 GMACs = 4.5747% MACs, 12.53 GFLOPS = 4.6147% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.1239% Params, 1.48 GMACs = 1.094% MACs, 3 GFLOPS = 1.1058% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 0.2997% Params, 3.62 GMACs = 2.6852% MACs, 7.3 GFLOPS = 2.6873% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.12 GFLOPS = 0.4139% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 0.6475% Params, 7.78 GMACs = 5.7682% MACs, 15.77 GFLOPS = 5.807% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.1459% Params, 1.74 GMACs = 1.2929% MACs, 3.54 GFLOPS = 1.3035% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 0.3217% Params, 3.89 GMACs = 2.8841% MACs, 7.83 GFLOPS = 2.885% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.21 GFLOPS = 0.8155% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 4.6113% Params, 13.96 GMACs = 10.3429% MACs, 28.13 GFLOPS = 10.3569% FLOPs
      (0): InceptionBlock(
        95.17 K = 2.042% Params, 6.17 GMACs = 4.5747% MACs, 12.44 GFLOPS = 4.5808% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 0.4896% Params, 1.48 GMACs = 1.094% MACs, 2.98 GFLOPS = 1.0965% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 1.1926% Params, 3.62 GMACs = 2.6852% MACs, 7.27 GFLOPS = 2.678% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.1 GFLOPS = 0.4046% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 2.5693% Params, 7.78 GMACs = 5.7682% MACs, 15.67 GFLOPS = 5.77% FLOPs
        (branch1): ConvBlock(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 0.5774% Params, 1.74 GMACs = 1.2929% MACs, 3.51 GFLOPS = 1.2942% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 1.2805% Params, 3.89 GMACs = 2.8841% MACs, 7.81 GFLOPS = 2.8757% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 18.3627% Params, 13.96 GMACs = 10.3429% MACs, 28.02 GFLOPS = 10.3183% FLOPs
      (0): InceptionBlock(
        378.75 K = 8.1267% Params, 6.17 GMACs = 4.5747% MACs, 12.39 GFLOPS = 4.5638% FLOPs
        (branch1): ConvBlock(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
          (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 1.9458% Params, 1.48 GMACs = 1.094% MACs, 2.97 GFLOPS = 1.0919% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 4.7582% Params, 3.62 GMACs = 2.6852% MACs, 7.26 GFLOPS = 2.6734% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 10.236% Params, 7.78 GMACs = 5.7682% MACs, 15.62 GFLOPS = 5.7514% FLOPs
        (branch1): ConvBlock(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 2.2974% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2896% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 5.1097% Params, 3.89 GMACs = 2.8841% MACs, 7.8 GFLOPS = 2.8711% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.42 M = 73.2859% Params, 13.96 GMACs = 10.3429% MACs, 27.97 GFLOPS = 10.299% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.4244% Params, 6.17 GMACs = 4.5747% MACs, 12.37 GFLOPS = 4.5553% FLOPs
        (branch1): ConvBlock(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          361.6 K = 7.7587% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0896% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.89 K = 19.0081% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.6711% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3977% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 40.8615% Params, 7.78 GMACs = 5.7682% MACs, 15.59 GFLOPS = 5.7422% FLOPs
        (branch1): ConvBlock(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
          (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          427.14 K = 9.1649% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2873% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          951.42 K = 20.4143% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8688% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
            (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 6.68 MFLOPS = 0.0025% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 5.64 MFLOPS = 0.0021% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
Epoch 1/30
Train Loss: 3.0411 Train Accuracy: 0.2133
Val Loss: 3.1053 Val Accuracy: 0.1486
Epoch 2/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:02<00:00,  2.23it/s]
Epoch 2/30
Train Loss: 2.1363 Train Accuracy: 0.4117
Val Loss: 2.0348 Val Accuracy: 0.4054
Epoch 3/30: 100%|██████████| 42/42 [00:18<00:00,  2.27it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s]
Epoch 3/30
Train Loss: 1.5549 Train Accuracy: 0.5254
Val Loss: 2.1811 Val Accuracy: 0.4054
Epoch 4/30: 100%|██████████| 42/42 [00:20<00:00,  2.08it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  2.89it/s]
Epoch 4/30
Train Loss: 1.3417 Train Accuracy: 0.6015
Val Loss: 2.5241 Val Accuracy: 0.2669
Epoch 5/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  2.82it/s]
Epoch 5/30
Train Loss: 1.2204 Train Accuracy: 0.6216
Val Loss: 2.8311 Val Accuracy: 0.3412
Epoch 6/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  2.87it/s]
Epoch 6/30
Train Loss: 1.1386 Train Accuracy: 0.6439
Val Loss: 2.2032 Val Accuracy: 0.3615
Epoch 7/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]
Epoch 7/30
Train Loss: 1.0706 Train Accuracy: 0.6557
Val Loss: 2.3790 Val Accuracy: 0.4493
Epoch 8/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]
Epoch 8/30
Train Loss: 0.9085 Train Accuracy: 0.7129
Val Loss: 2.7227 Val Accuracy: 0.4189
Epoch 9/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]
Epoch 9/30
Train Loss: 0.9339 Train Accuracy: 0.6981
Val Loss: 2.5447 Val Accuracy: 0.4088
Epoch 10/30: 100%|██████████| 42/42 [00:18<00:00,  2.30it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  2.71it/s]
Epoch 10/30
Train Loss: 0.8677 Train Accuracy: 0.7208
Val Loss: 2.1919 Val Accuracy: 0.4865
Epoch 11/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 11/30
Train Loss: 0.7616 Train Accuracy: 0.7712
Val Loss: 2.1698 Val Accuracy: 0.4730
Epoch 12/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  3.26it/s]
Epoch 12/30
Train Loss: 0.6577 Train Accuracy: 0.8008
Val Loss: 2.4563 Val Accuracy: 0.4426
Epoch 13/30: 100%|██████████| 42/42 [00:18<00:00,  2.30it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 13/30
Train Loss: 0.7000 Train Accuracy: 0.7883
Val Loss: 1.3733 Val Accuracy: 0.6081
Epoch 14/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 14/30
Train Loss: 0.5223 Train Accuracy: 0.8402
Val Loss: 1.3947 Val Accuracy: 0.5946
Epoch 15/30: 100%|██████████| 42/42 [00:18<00:00,  2.31it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:02<00:00,  2.23it/s]
Epoch 15/30
Train Loss: 0.4743 Train Accuracy: 0.8470
Val Loss: 1.4646 Val Accuracy: 0.5980
Epoch 16/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 16/30
Train Loss: 0.4804 Train Accuracy: 0.8606
Val Loss: 1.6880 Val Accuracy: 0.5439
Epoch 17/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]
Epoch 17/30
Train Loss: 0.4534 Train Accuracy: 0.8523
Val Loss: 1.2748 Val Accuracy: 0.6385
Epoch 18/30: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 18/30
Train Loss: 0.4028 Train Accuracy: 0.8867
Val Loss: 1.3051 Val Accuracy: 0.6284
Epoch 19/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]
Epoch 19/30
Train Loss: 0.4013 Train Accuracy: 0.8754
Val Loss: 1.7156 Val Accuracy: 0.5169
Epoch 20/30: 100%|██████████| 42/42 [00:18<00:00,  2.31it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  2.86it/s]
Epoch 20/30
Train Loss: 0.3279 Train Accuracy: 0.8977
Val Loss: 1.4813 Val Accuracy: 0.6216
Epoch 21/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.35it/s]
Epoch 21/30
Train Loss: 0.3063 Train Accuracy: 0.9076
Val Loss: 1.3374 Val Accuracy: 0.6250
Epoch 22/30: 100%|██████████| 42/42 [00:18<00:00,  2.33it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]
Epoch 22/30
Train Loss: 0.2746 Train Accuracy: 0.9193
Val Loss: 1.1964 Val Accuracy: 0.6520
Epoch 23/30: 100%|██████████| 42/42 [00:18<00:00,  2.31it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  3.33it/s]
Epoch 23/30
Train Loss: 0.2383 Train Accuracy: 0.9330
Val Loss: 1.2691 Val Accuracy: 0.6486
Epoch 24/30: 100%|██████████| 42/42 [00:17<00:00,  2.34it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.01it/s]
Epoch 24/30
Train Loss: 0.2177 Train Accuracy: 0.9379
Val Loss: 1.1886 Val Accuracy: 0.6554
Epoch 25/30: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.16it/s]
Epoch 25/30
Train Loss: 0.1798 Train Accuracy: 0.9477
Val Loss: 1.2821 Val Accuracy: 0.6520
Epoch 26/30: 100%|██████████| 42/42 [00:17<00:00,  2.35it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.32it/s]
Epoch 26/30
Train Loss: 0.1660 Train Accuracy: 0.9572
Val Loss: 1.1343 Val Accuracy: 0.6791
Epoch 27/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]
Epoch 27/30
Train Loss: 0.1552 Train Accuracy: 0.9606
Val Loss: 1.1804 Val Accuracy: 0.6554
Epoch 28/30: 100%|██████████| 42/42 [00:17<00:00,  2.36it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 28/30
Train Loss: 0.1539 Train Accuracy: 0.9549
Val Loss: 1.1162 Val Accuracy: 0.6791
Epoch 29/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]
Epoch 29/30
Train Loss: 0.1369 Train Accuracy: 0.9633
Val Loss: 1.1541 Val Accuracy: 0.6723
Epoch 30/30: 100%|██████████| 42/42 [00:18<00:00,  2.32it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 30/30
Train Loss: 0.1327 Train Accuracy: 0.9617
Val Loss: 1.1476 Val Accuracy: 0.6824
Training and evaluating 7x7stem model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.59 M  
fwd MACs:                                                               59.1431 GMACs
fwd FLOPs:                                                              119.347 GFLOPS
fwd+bwd MACs:                                                           177.429 GMACs
fwd+bwd FLOPs:                                                          358.042 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.59 M = 100% Params, 59.14 GMACs = 100% MACs, 119.35 GFLOPS = 100% FLOPs
  (stem): Sequential(
    9.6 K = 0.209% Params, 9.56 GMACs = 16.1627% MACs, 19.44 GFLOPS = 16.2914% FLOPs
    (0): ConvBlock(
      9.6 K = 0.209% Params, 9.56 GMACs = 16.1627% MACs, 19.38 GFLOPS = 16.2369% FLOPs
      (conv2d): Conv2d(9.47 K = 0.2062% Params, 9.56 GMACs = 16.1627% MACs, 19.18 GFLOPS = 16.0735% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(1, 1))
      (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 130.06 MFLOPS = 0.109% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 65.03 MFLOPS = 0.0545% FLOPs, negative_slope=0.01)
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 65.03 MFLOPS = 0.0545% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 1.1799% Params, 13.53 GMACs = 22.8697% MACs, 27.46 GFLOPS = 23.0069% FLOPs
      (0): InceptionBlock(
        24.03 K = 0.5231% Params, 5.98 GMACs = 10.1154% MACs, 12.14 GFLOPS = 10.1753% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.0467% Params, 520.22 MMACs = 0.8796% MACs, 1.07 GFLOPS = 0.899% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0453% Params, 520.22 MMACs = 0.8796% MACs, 1.05 GFLOPS = 0.8786% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.1257% Params, 1.43 GMACs = 2.4189% MACs, 2.91 GFLOPS = 2.4383% FLOPs
          (0): ConvBlock(
            1.07 K = 0.0233% Params, 260.11 MMACs = 0.4398% MACs, 536.48 MFLOPS = 0.4495% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0226% Params, 260.11 MMACs = 0.4398% MACs, 524.29 MFLOPS = 0.4393% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.06 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1024% Params, 1.17 GMACs = 1.9791% MACs, 2.37 GFLOPS = 1.9888% FLOPs
            (conv2d): Conv2d(4.64 K = 0.101% Params, 1.17 GMACs = 1.9791% MACs, 2.35 GFLOPS = 1.9683% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 0.304% Params, 3.51 GMACs = 5.9373% MACs, 7.07 GFLOPS = 5.9254% FLOPs
          (0): ConvBlock(
            1.07 K = 0.0233% Params, 260.11 MMACs = 0.4398% MACs, 536.48 MFLOPS = 0.4495% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0226% Params, 260.11 MMACs = 0.4398% MACs, 524.29 MFLOPS = 0.4393% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.06 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2807% Params, 3.25 GMACs = 5.4975% MACs, 6.54 GFLOPS = 5.4759% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2793% Params, 3.25 GMACs = 5.4975% MACs, 6.51 GFLOPS = 5.4554% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.0467% Params, 520.22 MMACs = 0.8796% MACs, 1.09 GFLOPS = 0.9126% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.0467% Params, 520.22 MMACs = 0.8796% MACs, 1.07 GFLOPS = 0.899% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0453% Params, 520.22 MMACs = 0.8796% MACs, 1.05 GFLOPS = 0.8786% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 0.6568% Params, 7.54 GMACs = 12.7543% MACs, 15.28 GFLOPS = 12.8043% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.0912% Params, 1.04 GMACs = 1.7592% MACs, 2.11 GFLOPS = 1.7708% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0899% Params, 1.04 GMACs = 1.7592% MACs, 2.09 GFLOPS = 1.7504% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.148% Params, 1.69 GMACs = 2.8587% MACs, 3.43 GFLOPS = 2.8742% FLOPs
          (0): ConvBlock(
            2.1 K = 0.0456% Params, 520.22 MMACs = 0.8796% MACs, 1.06 GFLOPS = 0.8854% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0449% Params, 520.22 MMACs = 0.8796% MACs, 1.04 GFLOPS = 0.8752% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.06 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1024% Params, 1.17 GMACs = 1.9791% MACs, 2.37 GFLOPS = 1.9888% FLOPs
            (conv2d): Conv2d(4.64 K = 0.101% Params, 1.17 GMACs = 1.9791% MACs, 2.35 GFLOPS = 1.9683% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 0.3263% Params, 3.77 GMACs = 6.3771% MACs, 7.59 GFLOPS = 6.3613% FLOPs
          (0): ConvBlock(
            2.1 K = 0.0456% Params, 520.22 MMACs = 0.8796% MACs, 1.06 GFLOPS = 0.8854% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0449% Params, 520.22 MMACs = 0.8796% MACs, 1.04 GFLOPS = 0.8752% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.06 MFLOPS = 0.0034% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2807% Params, 3.25 GMACs = 5.4975% MACs, 6.54 GFLOPS = 5.4759% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2793% Params, 3.25 GMACs = 5.4975% MACs, 6.51 GFLOPS = 5.4554% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.0912% Params, 1.04 GMACs = 1.7592% MACs, 2.15 GFLOPS = 1.7981% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 32.51 MFLOPS = 0.0272% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.0912% Params, 1.04 GMACs = 1.7592% MACs, 2.11 GFLOPS = 1.7708% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0899% Params, 1.04 GMACs = 1.7592% MACs, 2.09 GFLOPS = 1.7504% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.26 MFLOPS = 0.0136% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.13 MFLOPS = 0.0068% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 32.51 MFLOPS = 0.0272% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 4.6779% Params, 13.1 GMACs = 22.1494% MACs, 26.4 GFLOPS = 22.1174% FLOPs
      (0): InceptionBlock(
        95.17 K = 2.0715% Params, 5.79 GMACs = 9.7969% MACs, 11.67 GFLOPS = 9.7823% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.1825% Params, 503.84 MMACs = 0.8519% MACs, 1.02 GFLOPS = 0.8575% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1797% Params, 503.84 MMACs = 0.8519% MACs, 1.01 GFLOPS = 0.8476% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 0.4966% Params, 1.39 GMACs = 2.3427% MACs, 2.79 GFLOPS = 2.3417% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0912% Params, 251.92 MMACs = 0.426% MACs, 511.71 MFLOPS = 0.4288% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0899% Params, 251.92 MMACs = 0.426% MACs, 505.81 MFLOPS = 0.4238% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.97 MFLOPS = 0.0016% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.4054% Params, 1.13 GMACs = 1.9168% MACs, 2.28 GFLOPS = 1.9129% FLOPs
            (conv2d): Conv2d(18.5 K = 0.4026% Params, 1.13 GMACs = 1.9168% MACs, 2.27 GFLOPS = 1.903% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 1.2099% Params, 3.4 GMACs = 5.7503% MACs, 6.83 GFLOPS = 5.719% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0912% Params, 251.92 MMACs = 0.426% MACs, 511.71 MFLOPS = 0.4288% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0899% Params, 251.92 MMACs = 0.426% MACs, 505.81 MFLOPS = 0.4238% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.97 MFLOPS = 0.0016% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1186% Params, 3.15 GMACs = 5.3244% MACs, 6.31 GFLOPS = 5.2902% FLOPs
            (conv2d): Conv2d(51.26 K = 1.1159% Params, 3.15 GMACs = 5.3244% MACs, 6.3 GFLOPS = 5.2803% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.1825% Params, 503.84 MMACs = 0.8519% MACs, 1.03 GFLOPS = 0.8641% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.1825% Params, 503.84 MMACs = 0.8519% MACs, 1.02 GFLOPS = 0.8575% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1797% Params, 503.84 MMACs = 0.8519% MACs, 1.01 GFLOPS = 0.8476% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 2.6064% Params, 7.31 GMACs = 12.3526% MACs, 14.71 GFLOPS = 12.3219% FLOPs
        (branch1): ConvBlock(
          16.58 K = 0.3608% Params, 1.01 GMACs = 1.7038% MACs, 2.03 GFLOPS = 1.7018% FLOPs
          (conv2d): Conv2d(16.45 K = 0.358% Params, 1.01 GMACs = 1.7038% MACs, 2.02 GFLOPS = 1.692% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 0.5858% Params, 1.64 GMACs = 2.7687% MACs, 3.3 GFLOPS = 2.7638% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1804% Params, 503.84 MMACs = 0.8519% MACs, 1.02 GFLOPS = 0.8509% FLOPs
            (conv2d): Conv2d(8.22 K = 0.179% Params, 503.84 MMACs = 0.8519% MACs, 1.01 GFLOPS = 0.846% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.97 MFLOPS = 0.0016% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.4054% Params, 1.13 GMACs = 1.9168% MACs, 2.28 GFLOPS = 1.9129% FLOPs
            (conv2d): Conv2d(18.5 K = 0.4026% Params, 1.13 GMACs = 1.9168% MACs, 2.27 GFLOPS = 1.903% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 1.299% Params, 3.65 GMACs = 6.1763% MACs, 7.33 GFLOPS = 6.1412% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1804% Params, 503.84 MMACs = 0.8519% MACs, 1.02 GFLOPS = 0.8509% FLOPs
            (conv2d): Conv2d(8.22 K = 0.179% Params, 503.84 MMACs = 0.8519% MACs, 1.01 GFLOPS = 0.846% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.97 MFLOPS = 0.0016% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1186% Params, 3.15 GMACs = 5.3244% MACs, 6.31 GFLOPS = 5.2902% FLOPs
            (conv2d): Conv2d(51.26 K = 1.1159% Params, 3.15 GMACs = 5.3244% MACs, 6.3 GFLOPS = 5.2803% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 0.3608% Params, 1.01 GMACs = 1.7038% MACs, 2.05 GFLOPS = 1.715% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.75 MFLOPS = 0.0132% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 0.3608% Params, 1.01 GMACs = 1.7038% MACs, 2.03 GFLOPS = 1.7018% FLOPs
            (conv2d): Conv2d(16.45 K = 0.358% Params, 1.01 GMACs = 1.7038% MACs, 2.02 GFLOPS = 1.692% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 7.87 MFLOPS = 0.0066% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.94 MFLOPS = 0.0033% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.75 MFLOPS = 0.0132% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 18.6282% Params, 12.27 GMACs = 20.7435% MACs, 24.63 GFLOPS = 20.6363% FLOPs
      (0): InceptionBlock(
        378.75 K = 8.2442% Params, 5.43 GMACs = 9.175% MACs, 10.89 GFLOPS = 9.1274% FLOPs
        (branch1): ConvBlock(
          33.15 K = 0.7216% Params, 471.86 MMACs = 0.7978% MACs, 951.09 MFLOPS = 0.7969% FLOPs
          (conv2d): Conv2d(32.9 K = 0.716% Params, 471.86 MMACs = 0.7978% MACs, 945.56 MFLOPS = 0.7923% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 1.974% Params, 1.3 GMACs = 2.194% MACs, 2.61 GFLOPS = 2.1838% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3608% Params, 235.93 MMACs = 0.3989% MACs, 475.55 MFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(16.45 K = 0.358% Params, 235.93 MMACs = 0.3989% MACs, 472.78 MFLOPS = 0.3961% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 921.6 KFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.6132% Params, 1.06 GMACs = 1.7951% MACs, 2.13 GFLOPS = 1.7853% FLOPs
            (conv2d): Conv2d(73.86 K = 1.6076% Params, 1.06 GMACs = 1.7951% MACs, 2.13 GFLOPS = 1.7807% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 4.827% Params, 3.19 GMACs = 5.3853% MACs, 6.38 GFLOPS = 5.3467% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3608% Params, 235.93 MMACs = 0.3989% MACs, 475.55 MFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(16.45 K = 0.358% Params, 235.93 MMACs = 0.3989% MACs, 472.78 MFLOPS = 0.3961% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 921.6 KFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4662% Params, 2.95 GMACs = 4.9864% MACs, 5.91 GFLOPS = 4.9483% FLOPs
            (conv2d): Conv2d(204.93 K = 4.4606% Params, 2.95 GMACs = 4.9864% MACs, 5.9 GFLOPS = 4.9436% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 0.7216% Params, 471.86 MMACs = 0.7978% MACs, 954.78 MFLOPS = 0.8% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 0.7216% Params, 471.86 MMACs = 0.7978% MACs, 951.09 MFLOPS = 0.7969% FLOPs
            (conv2d): Conv2d(32.9 K = 0.716% Params, 471.86 MMACs = 0.7978% MACs, 945.56 MFLOPS = 0.7923% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 10.384% Params, 6.84 GMACs = 11.5685% MACs, 13.73 GFLOPS = 11.5027% FLOPs
        (branch1): ConvBlock(
          65.92 K = 1.4349% Params, 943.72 MMACs = 1.5957% MACs, 1.89 GFLOPS = 1.5876% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4293% Params, 943.72 MMACs = 1.5957% MACs, 1.89 GFLOPS = 1.583% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 2.3306% Params, 1.53 GMACs = 2.5929% MACs, 3.08 GFLOPS = 2.5791% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7174% Params, 471.86 MMACs = 0.7978% MACs, 947.4 MFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7146% Params, 471.86 MMACs = 0.7978% MACs, 944.64 MFLOPS = 0.7915% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 921.6 KFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.6132% Params, 1.06 GMACs = 1.7951% MACs, 2.13 GFLOPS = 1.7853% FLOPs
            (conv2d): Conv2d(73.86 K = 1.6076% Params, 1.06 GMACs = 1.7951% MACs, 2.13 GFLOPS = 1.7807% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 5.1836% Params, 3.42 GMACs = 5.7842% MACs, 6.85 GFLOPS = 5.7421% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7174% Params, 471.86 MMACs = 0.7978% MACs, 947.4 MFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7146% Params, 471.86 MMACs = 0.7978% MACs, 944.64 MFLOPS = 0.7915% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0028% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 921.6 KFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4662% Params, 2.95 GMACs = 4.9864% MACs, 5.91 GFLOPS = 4.9483% FLOPs
            (conv2d): Conv2d(204.93 K = 4.4606% Params, 2.95 GMACs = 4.9864% MACs, 5.9 GFLOPS = 4.9436% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 1.4349% Params, 943.72 MMACs = 1.5957% MACs, 1.9 GFLOPS = 1.5938% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.37 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 1.4349% Params, 943.72 MMACs = 1.5957% MACs, 1.89 GFLOPS = 1.5876% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4293% Params, 943.72 MMACs = 1.5957% MACs, 1.89 GFLOPS = 1.583% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 3.69 MFLOPS = 0.0031% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.37 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.42 M = 74.3456% Params, 10.69 GMACs = 18.0699% MACs, 21.41 GFLOPS = 17.9428% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.8933% Params, 4.73 GMACs = 7.9924% MACs, 9.47 GFLOPS = 7.9362% FLOPs
        (branch1): ConvBlock(
          131.84 K = 2.8697% Params, 411.04 MMACs = 0.695% MACs, 825.29 MFLOPS = 0.6915% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8586% Params, 411.04 MMACs = 0.695% MACs, 822.89 MFLOPS = 0.6895% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          361.6 K = 7.8709% Params, 1.13 GMACs = 1.9112% MACs, 2.27 GFLOPS = 1.8983% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4349% Params, 205.52 MMACs = 0.3475% MACs, 412.65 MFLOPS = 0.3458% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4293% Params, 205.52 MMACs = 0.3475% MACs, 411.44 MFLOPS = 0.3447% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0003% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.436% Params, 924.84 MMACs = 1.5637% MACs, 1.85 GFLOPS = 1.5525% FLOPs
            (conv2d): Conv2d(295.17 K = 6.4249% Params, 924.84 MMACs = 1.5637% MACs, 1.85 GFLOPS = 1.5505% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.89 K = 19.2829% Params, 2.77 GMACs = 4.6912% MACs, 5.55 GFLOPS = 4.6535% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4349% Params, 205.52 MMACs = 0.3475% MACs, 412.65 MFLOPS = 0.3458% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4293% Params, 205.52 MMACs = 0.3475% MACs, 411.44 MFLOPS = 0.3447% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0003% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.8481% Params, 2.57 GMACs = 4.3437% MACs, 5.14 GFLOPS = 4.3078% FLOPs
            (conv2d): Conv2d(819.46 K = 17.8369% Params, 2.57 GMACs = 4.3437% MACs, 5.14 GFLOPS = 4.3058% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.84 K = 2.8697% Params, 411.04 MMACs = 0.695% MACs, 826.9 MFLOPS = 0.6929% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.84 K = 2.8697% Params, 411.04 MMACs = 0.695% MACs, 825.29 MFLOPS = 0.6915% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8586% Params, 411.04 MMACs = 0.695% MACs, 822.89 MFLOPS = 0.6895% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 41.4523% Params, 5.96 GMACs = 10.0774% MACs, 11.94 GFLOPS = 10.004% FLOPs
        (branch1): ConvBlock(
          262.91 K = 5.7227% Params, 822.08 MMACs = 1.39% MACs, 1.65 GFLOPS = 1.3803% FLOPs
          (conv2d): Conv2d(262.4 K = 5.7116% Params, 822.08 MMACs = 1.39% MACs, 1.64 GFLOPS = 1.3783% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          427.14 K = 9.2974% Params, 1.34 GMACs = 2.2587% MACs, 2.68 GFLOPS = 2.2427% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8614% Params, 411.04 MMACs = 0.695% MACs, 823.69 MFLOPS = 0.6902% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8558% Params, 411.04 MMACs = 0.695% MACs, 822.48 MFLOPS = 0.6892% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0003% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.436% Params, 924.84 MMACs = 1.5637% MACs, 1.85 GFLOPS = 1.5525% FLOPs
            (conv2d): Conv2d(295.17 K = 6.4249% Params, 924.84 MMACs = 1.5637% MACs, 1.85 GFLOPS = 1.5505% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          951.42 K = 20.7094% Params, 2.98 GMACs = 5.0387% MACs, 5.96 GFLOPS = 4.9979% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8614% Params, 411.04 MMACs = 0.695% MACs, 823.69 MFLOPS = 0.6902% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8558% Params, 411.04 MMACs = 0.695% MACs, 822.48 MFLOPS = 0.6892% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0056% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0003% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.8481% Params, 2.57 GMACs = 4.3437% MACs, 5.14 GFLOPS = 4.3078% FLOPs
            (conv2d): Conv2d(819.46 K = 17.8369% Params, 2.57 GMACs = 4.3437% MACs, 5.14 GFLOPS = 4.3058% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.91 K = 5.7227% Params, 822.08 MMACs = 1.39% MACs, 1.65 GFLOPS = 1.383% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 3.21 MFLOPS = 0.0027% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.91 K = 5.7227% Params, 822.08 MMACs = 1.39% MACs, 1.65 GFLOPS = 1.3803% FLOPs
            (conv2d): Conv2d(262.4 K = 5.7116% Params, 822.08 MMACs = 1.39% MACs, 1.64 GFLOPS = 1.3783% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.0111% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0013% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0007% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 3.21 MFLOPS = 0.0027% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9594% Params, 2.82 MMACs = 0.0048% MACs, 6.23 MFLOPS = 0.0052% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 589.82 KFLOPS = 0.0005% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(44.08 K = 0.9594% Params, 2.82 MMACs = 0.0048% MACs, 5.64 MFLOPS = 0.0047% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:14<00:00,  2.88it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s]
Epoch 1/30
Train Loss: 3.1779 Train Accuracy: 0.1758
Val Loss: 2.8932 Val Accuracy: 0.1892
Epoch 2/30: 100%|██████████| 42/42 [00:14<00:00,  2.92it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:01<00:00,  3.53it/s]
Epoch 2/30
Train Loss: 2.3378 Train Accuracy: 0.3333
Val Loss: 2.5071 Val Accuracy: 0.2770
Epoch 3/30: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:02<00:00,  2.28it/s]
Epoch 3/30
Train Loss: 1.7888 Train Accuracy: 0.4648
Val Loss: 2.3446 Val Accuracy: 0.3851
Epoch 4/30: 100%|██████████| 42/42 [00:14<00:00,  2.92it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  3.46it/s]
Epoch 4/30
Train Loss: 1.5244 Train Accuracy: 0.5352
Val Loss: 1.9941 Val Accuracy: 0.4527
Epoch 5/30: 100%|██████████| 42/42 [00:14<00:00,  2.86it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  3.60it/s]
Epoch 5/30
Train Loss: 1.3852 Train Accuracy: 0.5754
Val Loss: 2.1584 Val Accuracy: 0.3953
Epoch 6/30: 100%|██████████| 42/42 [00:14<00:00,  2.88it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  3.43it/s]
Epoch 6/30
Train Loss: 1.2715 Train Accuracy: 0.5951
Val Loss: 2.7301 Val Accuracy: 0.2939
Epoch 7/30: 100%|██████████| 42/42 [00:14<00:00,  2.91it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.56it/s]
Epoch 7/30
Train Loss: 1.2642 Train Accuracy: 0.6027
Val Loss: 3.1149 Val Accuracy: 0.2838
Epoch 8/30: 100%|██████████| 42/42 [00:14<00:00,  2.88it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:01<00:00,  3.54it/s]
Epoch 8/30
Train Loss: 1.1678 Train Accuracy: 0.6322
Val Loss: 1.9485 Val Accuracy: 0.5000
Epoch 9/30: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  2.71it/s]
Epoch 9/30
Train Loss: 1.0051 Train Accuracy: 0.6761
Val Loss: 2.4558 Val Accuracy: 0.3716
Epoch 10/30: 100%|██████████| 42/42 [00:15<00:00,  2.78it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 10/30
Train Loss: 0.8813 Train Accuracy: 0.7216
Val Loss: 1.7123 Val Accuracy: 0.4662
Epoch 11/30: 100%|██████████| 42/42 [00:14<00:00,  2.92it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s]
Epoch 11/30
Train Loss: 0.8153 Train Accuracy: 0.7311
Val Loss: 1.9651 Val Accuracy: 0.4932
Epoch 12/30: 100%|██████████| 42/42 [00:14<00:00,  2.90it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  3.48it/s]
Epoch 12/30
Train Loss: 0.7143 Train Accuracy: 0.7716
Val Loss: 1.9137 Val Accuracy: 0.4932
Epoch 13/30: 100%|██████████| 42/42 [00:14<00:00,  2.89it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s]
Epoch 13/30
Train Loss: 0.6341 Train Accuracy: 0.8049
Val Loss: 1.8034 Val Accuracy: 0.5473
Epoch 14/30: 100%|██████████| 42/42 [00:14<00:00,  2.87it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  3.47it/s]
Epoch 14/30
Train Loss: 0.6720 Train Accuracy: 0.7962
Val Loss: 2.0078 Val Accuracy: 0.4932
Epoch 15/30: 100%|██████████| 42/42 [00:14<00:00,  2.83it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s]
Epoch 15/30
Train Loss: 0.6410 Train Accuracy: 0.8008
Val Loss: 1.8382 Val Accuracy: 0.4730
Epoch 16/30: 100%|██████████| 42/42 [00:15<00:00,  2.76it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]
Epoch 16/30
Train Loss: 0.5653 Train Accuracy: 0.8208
Val Loss: 1.3715 Val Accuracy: 0.6115
Epoch 17/30: 100%|██████████| 42/42 [00:14<00:00,  2.90it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.39it/s]
Epoch 17/30
Train Loss: 0.4917 Train Accuracy: 0.8485
Val Loss: 1.4730 Val Accuracy: 0.6115
Epoch 18/30: 100%|██████████| 42/42 [00:14<00:00,  2.87it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  3.43it/s]
Epoch 18/30
Train Loss: 0.4184 Train Accuracy: 0.8735
Val Loss: 1.3890 Val Accuracy: 0.5946
Epoch 19/30: 100%|██████████| 42/42 [00:14<00:00,  2.92it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.46it/s]
Epoch 19/30
Train Loss: 0.3802 Train Accuracy: 0.8818
Val Loss: 1.5088 Val Accuracy: 0.5642
Epoch 20/30: 100%|██████████| 42/42 [00:14<00:00,  2.88it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  3.50it/s]
Epoch 20/30
Train Loss: 0.3654 Train Accuracy: 0.8909
Val Loss: 1.5107 Val Accuracy: 0.6149
Epoch 21/30: 100%|██████████| 42/42 [00:14<00:00,  2.89it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.68it/s]
Epoch 21/30
Train Loss: 0.3468 Train Accuracy: 0.8962
Val Loss: 1.4342 Val Accuracy: 0.6182
Epoch 22/30: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s]
Epoch 22/30
Train Loss: 0.3229 Train Accuracy: 0.9004
Val Loss: 1.6049 Val Accuracy: 0.5946
Epoch 23/30: 100%|██████████| 42/42 [00:14<00:00,  2.85it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]
Epoch 23/30
Train Loss: 0.2809 Train Accuracy: 0.9170
Val Loss: 1.2307 Val Accuracy: 0.6622
Epoch 24/30: 100%|██████████| 42/42 [00:14<00:00,  2.86it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.56it/s]
Epoch 24/30
Train Loss: 0.2474 Train Accuracy: 0.9375
Val Loss: 1.3436 Val Accuracy: 0.6351
Epoch 25/30: 100%|██████████| 42/42 [00:14<00:00,  2.92it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.51it/s]
Epoch 25/30
Train Loss: 0.2281 Train Accuracy: 0.9330
Val Loss: 1.3289 Val Accuracy: 0.6351
Epoch 26/30: 100%|██████████| 42/42 [00:14<00:00,  2.90it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s]
Epoch 26/30
Train Loss: 0.2041 Train Accuracy: 0.9383
Val Loss: 1.2216 Val Accuracy: 0.6453
Epoch 27/30: 100%|██████████| 42/42 [00:14<00:00,  2.90it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.45it/s]
Epoch 27/30
Train Loss: 0.2168 Train Accuracy: 0.9383
Val Loss: 1.2982 Val Accuracy: 0.6520
Epoch 28/30: 100%|██████████| 42/42 [00:14<00:00,  2.83it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s]
Epoch 28/30
Train Loss: 0.1998 Train Accuracy: 0.9420
Val Loss: 1.2349 Val Accuracy: 0.6520
Epoch 29/30: 100%|██████████| 42/42 [00:14<00:00,  2.80it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 29/30
Train Loss: 0.1818 Train Accuracy: 0.9496
Val Loss: 1.3064 Val Accuracy: 0.6419
Epoch 30/30: 100%|██████████| 42/42 [00:14<00:00,  2.87it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s]
Epoch 30/30
Train Loss: 0.1750 Train Accuracy: 0.9538
Val Loss: 1.3074 Val Accuracy: 0.6385
Training and evaluating no_bm model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.65 M  
fwd MACs:                                                               134.959 GMACs
fwd FLOPs:                                                              270.86 GFLOPS
fwd+bwd MACs:                                                           404.876 GMACs
fwd+bwd FLOPs:                                                          812.581 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.65 M = 100% Params, 134.96 GMACs = 100% MACs, 270.86 GFLOPS = 100% FLOPs
  (stem): Sequential(
    75.65 K = 1.6266% Params, 79.12 GMACs = 58.6263% MACs, 158.71 GFLOPS = 58.5957% FLOPs
    (0): Sequential(
      75.65 K = 1.6266% Params, 79.12 GMACs = 58.6263% MACs, 158.65 GFLOPS = 58.5709% FLOPs
      (0): ConvBlock(
        1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.76 GFLOPS = 1.3875% FLOPs
        (conv2d): Conv2d(1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.69 GFLOPS = 1.3627% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0248% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        36.93 K = 0.794% Params, 38.65 GMACs = 28.6419% MACs, 77.44 GFLOPS = 28.5917% FLOPs
        (conv2d): Conv2d(36.93 K = 0.794% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.567% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0248% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        36.93 K = 0.794% Params, 38.65 GMACs = 28.6419% MACs, 77.44 GFLOPS = 28.5917% FLOPs
        (conv2d): Conv2d(36.93 K = 0.794% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.567% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0248% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0248% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      53.57 K = 1.1519% Params, 13.96 GMACs = 10.3429% MACs, 28.17 GFLOPS = 10.3998% FLOPs
      (0): InceptionBlock(
        23.71 K = 0.5099% Params, 6.17 GMACs = 4.5747% MACs, 12.45 GFLOPS = 4.596% FLOPs
        (branch1): ConvBlock(
          2.08 K = 0.0447% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4026% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0447% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.68 K = 0.1221% Params, 1.48 GMACs = 1.094% MACs, 2.98 GFLOPS = 1.0994% FLOPs
          (0): ConvBlock(
            1.04 K = 0.0224% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2013% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0224% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1998% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.64 K = 0.0998% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8981% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0998% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.895% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.87 K = 0.2983% Params, 3.62 GMACs = 2.6852% MACs, 7.27 GFLOPS = 2.6851% FLOPs
          (0): ConvBlock(
            1.04 K = 0.0224% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2013% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0224% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1998% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.83 K = 0.2759% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4838% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2759% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4807% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.08 K = 0.0447% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4088% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.08 K = 0.0447% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4026% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0447% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        29.86 K = 0.642% Params, 7.78 GMACs = 5.7682% MACs, 15.69 GFLOPS = 5.7914% FLOPs
        (branch1): ConvBlock(
          4.13 K = 0.0888% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.799% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0888% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7959% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.7 K = 0.1442% Params, 1.74 GMACs = 1.2929% MACs, 3.51 GFLOPS = 1.2977% FLOPs
          (0): ConvBlock(
            2.06 K = 0.0444% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0444% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.64 K = 0.0998% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8981% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0998% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.895% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.9 K = 0.3203% Params, 3.89 GMACs = 2.8841% MACs, 7.81 GFLOPS = 2.8833% FLOPs
          (0): ConvBlock(
            2.06 K = 0.0444% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0444% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.83 K = 0.2759% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4838% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2759% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4807% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.13 K = 0.0888% Params, 1.07 GMACs = 0.7956% MACs, 2.2 GFLOPS = 0.8114% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.13 K = 0.0888% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.799% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0888% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7959% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      213.63 K = 4.5936% Params, 13.96 GMACs = 10.3429% MACs, 28.04 GFLOPS = 10.3534% FLOPs
      (0): InceptionBlock(
        94.53 K = 2.0326% Params, 6.17 GMACs = 4.5747% MACs, 12.4 GFLOPS = 4.5774% FLOPs
        (branch1): ConvBlock(
          8.26 K = 0.1775% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1775% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.62 K = 0.4865% Params, 1.48 GMACs = 1.094% MACs, 2.97 GFLOPS = 1.0948% FLOPs
          (0): ConvBlock(
            4.13 K = 0.0888% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1998% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0888% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.199% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.5 K = 0.3977% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.895% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3977% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8935% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.39 K = 1.1911% Params, 3.62 GMACs = 2.6852% MACs, 7.26 GFLOPS = 2.6805% FLOPs
          (0): ConvBlock(
            4.13 K = 0.0888% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1998% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0888% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.199% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.26 K = 1.1023% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4807% FLOPs
            (conv2d): Conv2d(51.26 K = 1.1023% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4792% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.26 K = 0.1775% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4026% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.26 K = 0.1775% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1775% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.1 K = 2.561% Params, 7.78 GMACs = 5.7682% MACs, 15.63 GFLOPS = 5.7698% FLOPs
        (branch1): ConvBlock(
          16.45 K = 0.3537% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7959% FLOPs
          (conv2d): Conv2d(16.45 K = 0.3537% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7944% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.72 K = 0.5745% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.293% FLOPs
          (0): ConvBlock(
            8.22 K = 0.1768% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1768% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.5 K = 0.3977% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.895% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3977% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8935% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.49 K = 1.2791% Params, 3.89 GMACs = 2.8841% MACs, 7.8 GFLOPS = 2.8787% FLOPs
          (0): ConvBlock(
            8.22 K = 0.1768% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1768% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.26 K = 1.1023% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4807% FLOPs
            (conv2d): Conv2d(51.26 K = 1.1023% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4792% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.45 K = 0.3537% Params, 1.07 GMACs = 0.7956% MACs, 2.17 GFLOPS = 0.8021% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.45 K = 0.3537% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7959% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3537% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7944% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      853.25 K = 18.347% Params, 13.96 GMACs = 10.3429% MACs, 27.98 GFLOPS = 10.3301% FLOPs
      (0): InceptionBlock(
        377.47 K = 8.1166% Params, 6.17 GMACs = 4.5747% MACs, 12.37 GFLOPS = 4.5681% FLOPs
        (branch1): ConvBlock(
          32.9 K = 0.7073% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs
          (conv2d): Conv2d(32.9 K = 0.7073% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.3 K = 1.9418% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0925% FLOPs
          (0): ConvBlock(
            16.45 K = 0.3537% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.199% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3537% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1986% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            73.86 K = 1.5881% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8935% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5881% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.38 K = 4.7602% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.6782% FLOPs
          (0): ConvBlock(
            16.45 K = 0.3537% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.199% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3537% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1986% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            204.93 K = 4.4065% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4792% FLOPs
            (conv2d): Conv2d(204.93 K = 4.4065% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4784% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          32.9 K = 0.7073% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3995% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            32.9 K = 0.7073% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs
            (conv2d): Conv2d(32.9 K = 0.7073% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        475.78 K = 10.2304% Params, 7.78 GMACs = 5.7682% MACs, 15.6 GFLOPS = 5.7589% FLOPs
        (branch1): ConvBlock(
          65.66 K = 1.4119% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7944% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4119% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7936% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          106.69 K = 2.2941% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2907% FLOPs
          (0): ConvBlock(
            32.83 K = 0.706% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs
            (conv2d): Conv2d(32.83 K = 0.706% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            73.86 K = 1.5881% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8935% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5881% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          237.76 K = 5.1125% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8764% FLOPs
          (0): ConvBlock(
            32.83 K = 0.706% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs
            (conv2d): Conv2d(32.83 K = 0.706% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            204.93 K = 4.4065% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4792% FLOPs
            (conv2d): Conv2d(204.93 K = 4.4065% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4784% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.66 K = 1.4119% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7975% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.66 K = 1.4119% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7944% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4119% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7936% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.41 M = 73.3331% Params, 13.96 GMACs = 10.3429% MACs, 27.95 GFLOPS = 10.3185% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.439% Params, 6.17 GMACs = 4.5747% MACs, 12.36 GFLOPS = 4.5635% FLOPs
        (branch1): ConvBlock(
          131.33 K = 2.8239% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8239% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          360.83 K = 7.7588% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0913% FLOPs
          (0): ConvBlock(
            65.66 K = 1.4119% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1986% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4119% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1984% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.17 K = 6.3469% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3469% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8923% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.12 K = 19.0324% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.677% FLOPs
          (0): ConvBlock(
            65.66 K = 1.4119% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1986% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4119% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1984% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.46 K = 17.6204% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4784% FLOPs
            (conv2d): Conv2d(819.46 K = 17.6204% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.478% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.33 K = 2.8239% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.398% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.33 K = 2.8239% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3972% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8239% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 40.8941% Params, 7.78 GMACs = 5.7682% MACs, 15.58 GFLOPS = 5.7535% FLOPs
        (branch1): ConvBlock(
          262.4 K = 5.6423% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7936% FLOPs
          (conv2d): Conv2d(262.4 K = 5.6423% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7932% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          426.37 K = 9.168% Params, 1.74 GMACs = 1.2929% MACs, 3.49 GFLOPS = 1.2895% FLOPs
          (0): ConvBlock(
            131.2 K = 2.8211% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8211% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3966% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.17 K = 6.3469% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3469% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8923% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          950.66 K = 20.4416% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8752% FLOPs
          (0): ConvBlock(
            131.2 K = 2.8211% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3968% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8211% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3966% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.46 K = 17.6204% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4784% FLOPs
            (conv2d): Conv2d(819.46 K = 17.6204% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.478% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.4 K = 5.6423% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7952% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.4 K = 5.6423% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7936% FLOPs
            (conv2d): Conv2d(262.4 K = 5.6423% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7932% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9477% Params, 2.82 MMACs = 0.0021% MACs, 6.68 MFLOPS = 0.0025% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(44.08 K = 0.9477% Params, 2.82 MMACs = 0.0021% MACs, 5.64 MFLOPS = 0.0021% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:17<00:00,  2.46it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
Epoch 1/30
Train Loss: 3.7255 Train Accuracy: 0.0773
Val Loss: 3.6640 Val Accuracy: 0.0743
Epoch 2/30: 100%|██████████| 42/42 [00:16<00:00,  2.48it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:02<00:00,  2.18it/s]
Epoch 2/30
Train Loss: 3.4699 Train Accuracy: 0.0951
Val Loss: 3.5027 Val Accuracy: 0.0743
Epoch 3/30: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 3/30
Train Loss: 3.4267 Train Accuracy: 0.1015
Val Loss: 3.5082 Val Accuracy: 0.0980
Epoch 4/30: 100%|██████████| 42/42 [00:16<00:00,  2.47it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 4/30
Train Loss: 3.4311 Train Accuracy: 0.1030
Val Loss: 3.4975 Val Accuracy: 0.0980
Epoch 5/30: 100%|██████████| 42/42 [00:17<00:00,  2.47it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]
Epoch 5/30
Train Loss: 3.4305 Train Accuracy: 0.0951
Val Loss: 3.4967 Val Accuracy: 0.0743
Epoch 6/30: 100%|██████████| 42/42 [00:17<00:00,  2.40it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
Epoch 6/30
Train Loss: 3.3393 Train Accuracy: 0.1121
Val Loss: 3.1911 Val Accuracy: 0.1284
Epoch 7/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]
Epoch 7/30
Train Loss: 3.0432 Train Accuracy: 0.1523
Val Loss: 3.0809 Val Accuracy: 0.1723
Epoch 8/30: 100%|██████████| 42/42 [00:16<00:00,  2.48it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s]
Epoch 8/30
Train Loss: 2.9313 Train Accuracy: 0.1788
Val Loss: 3.0237 Val Accuracy: 0.1689
Epoch 9/30: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s]
Epoch 9/30
Train Loss: 2.8679 Train Accuracy: 0.1962
Val Loss: 3.1746 Val Accuracy: 0.1689
Epoch 10/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 10/30
Train Loss: 2.8265 Train Accuracy: 0.1955
Val Loss: 3.1339 Val Accuracy: 0.1655
Epoch 11/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  2.78it/s]
Epoch 11/30
Train Loss: 2.7176 Train Accuracy: 0.2261
Val Loss: 2.9688 Val Accuracy: 0.1892
Epoch 12/30: 100%|██████████| 42/42 [00:17<00:00,  2.39it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]
Epoch 12/30
Train Loss: 2.6324 Train Accuracy: 0.2307
Val Loss: 3.0091 Val Accuracy: 0.1622
Epoch 13/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.32it/s]
Epoch 13/30
Train Loss: 2.5914 Train Accuracy: 0.2470
Val Loss: 2.7583 Val Accuracy: 0.1926
Epoch 14/30: 100%|██████████| 42/42 [00:16<00:00,  2.50it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  2.85it/s]
Epoch 14/30
Train Loss: 2.5273 Train Accuracy: 0.2549
Val Loss: 2.8318 Val Accuracy: 0.1892
Epoch 15/30: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
Epoch 15/30
Train Loss: 2.4882 Train Accuracy: 0.2542
Val Loss: 2.8964 Val Accuracy: 0.1486
Epoch 16/30: 100%|██████████| 42/42 [00:16<00:00,  2.50it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:01<00:00,  3.29it/s]
Epoch 16/30
Train Loss: 2.4134 Train Accuracy: 0.2837
Val Loss: 2.7173 Val Accuracy: 0.2128
Epoch 17/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 17/30
Train Loss: 2.2760 Train Accuracy: 0.3152
Val Loss: 2.6364 Val Accuracy: 0.2736
Epoch 18/30: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 18/30
Train Loss: 2.2385 Train Accuracy: 0.3144
Val Loss: 2.5007 Val Accuracy: 0.2905
Epoch 19/30: 100%|██████████| 42/42 [00:17<00:00,  2.47it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]
Epoch 19/30
Train Loss: 2.1618 Train Accuracy: 0.3352
Val Loss: 2.4949 Val Accuracy: 0.2568
Epoch 20/30: 100%|██████████| 42/42 [00:16<00:00,  2.48it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  3.00it/s]
Epoch 20/30
Train Loss: 2.1003 Train Accuracy: 0.3413
Val Loss: 2.4337 Val Accuracy: 0.2872
Epoch 21/30: 100%|██████████| 42/42 [00:17<00:00,  2.40it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
Epoch 21/30
Train Loss: 2.0925 Train Accuracy: 0.3383
Val Loss: 2.4768 Val Accuracy: 0.2770
Epoch 22/30: 100%|██████████| 42/42 [00:16<00:00,  2.48it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 22/30
Train Loss: 2.0439 Train Accuracy: 0.3489
Val Loss: 2.5498 Val Accuracy: 0.2500
Epoch 23/30: 100%|██████████| 42/42 [00:16<00:00,  2.48it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]
Epoch 23/30
Train Loss: 1.9688 Train Accuracy: 0.3648
Val Loss: 2.3809 Val Accuracy: 0.2804
Epoch 24/30: 100%|██████████| 42/42 [00:17<00:00,  2.43it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.25it/s]
Epoch 24/30
Train Loss: 1.9451 Train Accuracy: 0.3727
Val Loss: 2.4249 Val Accuracy: 0.2939
Epoch 25/30: 100%|██████████| 42/42 [00:17<00:00,  2.46it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.20it/s]
Epoch 25/30
Train Loss: 1.9402 Train Accuracy: 0.3867
Val Loss: 2.4186 Val Accuracy: 0.2770
Epoch 26/30: 100%|██████████| 42/42 [00:16<00:00,  2.50it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 26/30
Train Loss: 1.9241 Train Accuracy: 0.3799
Val Loss: 2.3998 Val Accuracy: 0.2939
Epoch 27/30: 100%|██████████| 42/42 [00:17<00:00,  2.40it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]
Epoch 27/30
Train Loss: 1.8652 Train Accuracy: 0.4004
Val Loss: 2.3442 Val Accuracy: 0.2872
Epoch 28/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  3.19it/s]
Epoch 28/30
Train Loss: 1.8646 Train Accuracy: 0.3932
Val Loss: 2.3639 Val Accuracy: 0.2770
Epoch 29/30: 100%|██████████| 42/42 [00:16<00:00,  2.49it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  2.87it/s]
Epoch 29/30
Train Loss: 1.8704 Train Accuracy: 0.4011
Val Loss: 2.3798 Val Accuracy: 0.2736
Epoch 30/30: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s]
Epoch 30/30
Train Loss: 1.8409 Train Accuracy: 0.4000
Val Loss: 2.3763 Val Accuracy: 0.2601
Training and evaluating no_da model

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  4.66 M  
fwd MACs:                                                               134.959 GMACs
fwd FLOPs:                                                              271.577 GFLOPS
fwd+bwd MACs:                                                           404.876 GMACs
fwd+bwd FLOPs:                                                          814.732 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

GroceryStoreModel(
  4.66 M = 100% Params, 134.96 GMACs = 100% MACs, 271.58 GFLOPS = 100% FLOPs
  (stem): Sequential(
    76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.12 GFLOPS = 58.5892% FLOPs
    (0): Sequential(
      76.03 K = 1.6314% Params, 79.12 GMACs = 58.6263% MACs, 159.05 GFLOPS = 58.5645% FLOPs
      (0): ConvBlock(
        1.92 K = 0.0412% Params, 1.81 GMACs = 1.3426% MACs, 3.89 GFLOPS = 1.4332% FLOPs
        (conv2d): Conv2d(1.79 K = 0.0385% Params, 1.81 GMACs = 1.3426% MACs, 3.69 GFLOPS = 1.3591% FLOPs, 3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (1): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
      (2): ConvBlock(
        37.06 K = 0.7951% Params, 38.65 GMACs = 28.6419% MACs, 77.58 GFLOPS = 28.5657% FLOPs
        (conv2d): Conv2d(36.93 K = 0.7923% Params, 38.65 GMACs = 28.6419% MACs, 77.38 GFLOPS = 28.4915% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 134.22 MFLOPS = 0.0494% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, negative_slope=0.01)
      )
    )
    (1): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 67.11 MFLOPS = 0.0247% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (stages): ModuleList(
    (0): Sequential(
      54.21 K = 1.1631% Params, 13.96 GMACs = 10.3429% MACs, 28.34 GFLOPS = 10.4341% FLOPs
      (0): InceptionBlock(
        24.03 K = 0.5156% Params, 6.17 GMACs = 4.5747% MACs, 12.53 GFLOPS = 4.6147% FLOPs
        (branch1): ConvBlock(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
          (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          5.78 K = 0.1239% Params, 1.48 GMACs = 1.094% MACs, 3 GFLOPS = 1.1058% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          13.97 K = 0.2997% Params, 3.62 GMACs = 2.6852% MACs, 7.3 GFLOPS = 2.6873% FLOPs
          (0): ConvBlock(
            1.07 K = 0.023% Params, 268.44 MMACs = 0.1989% MACs, 553.65 MFLOPS = 0.2039% FLOPs
            (conv2d): Conv2d(1.04 K = 0.0223% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.12 GFLOPS = 0.4139% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            2.14 K = 0.046% Params, 536.87 MMACs = 0.3978% MACs, 1.11 GFLOPS = 0.4077% FLOPs
            (conv2d): Conv2d(2.08 K = 0.0446% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs, 64, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        30.18 K = 0.6475% Params, 7.78 GMACs = 5.7682% MACs, 15.77 GFLOPS = 5.807% FLOPs
        (branch1): ConvBlock(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          6.8 K = 0.1459% Params, 1.74 GMACs = 1.2929% MACs, 3.54 GFLOPS = 1.3035% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            4.7 K = 0.1009% Params, 1.21 GMACs = 0.8951% MACs, 2.45 GFLOPS = 0.9019% FLOPs
            (conv2d): Conv2d(4.64 K = 0.0996% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          14.99 K = 0.3217% Params, 3.89 GMACs = 2.8841% MACs, 7.83 GFLOPS = 2.885% FLOPs
          (0): ConvBlock(
            2.1 K = 0.045% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(2.06 K = 0.0443% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 16, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(32 = 0.0007% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            12.9 K = 0.2767% Params, 3.36 GMACs = 2.4863% MACs, 6.74 GFLOPS = 2.4834% FLOPs
            (conv2d): Conv2d(12.83 K = 0.2753% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs, 16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.21 GFLOPS = 0.8155% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            4.19 K = 0.0899% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 33.55 MFLOPS = 0.0124% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      214.91 K = 4.6113% Params, 13.96 GMACs = 10.3429% MACs, 28.13 GFLOPS = 10.3569% FLOPs
      (0): InceptionBlock(
        95.17 K = 2.042% Params, 6.17 GMACs = 4.5747% MACs, 12.44 GFLOPS = 4.5808% FLOPs
        (branch1): ConvBlock(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
          (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          22.82 K = 0.4896% Params, 1.48 GMACs = 1.094% MACs, 2.98 GFLOPS = 1.0965% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          55.58 K = 1.1926% Params, 3.62 GMACs = 2.6852% MACs, 7.27 GFLOPS = 2.678% FLOPs
          (0): ConvBlock(
            4.19 K = 0.0899% Params, 268.44 MMACs = 0.1989% MACs, 545.26 MFLOPS = 0.2008% FLOPs
            (conv2d): Conv2d(4.13 K = 0.0886% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs, 128, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.1 GFLOPS = 0.4046% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            8.38 K = 0.1799% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4016% FLOPs
            (conv2d): Conv2d(8.26 K = 0.1771% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs, 128, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        119.74 K = 2.5693% Params, 7.78 GMACs = 5.7682% MACs, 15.67 GFLOPS = 5.77% FLOPs
        (branch1): ConvBlock(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          26.91 K = 0.5774% Params, 1.74 GMACs = 1.2929% MACs, 3.51 GFLOPS = 1.2942% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            18.62 K = 0.3996% Params, 1.21 GMACs = 0.8951% MACs, 2.43 GFLOPS = 0.8958% FLOPs
            (conv2d): Conv2d(18.5 K = 0.3969% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          59.68 K = 1.2805% Params, 3.89 GMACs = 2.8841% MACs, 7.81 GFLOPS = 2.8757% FLOPs
          (0): ConvBlock(
            8.29 K = 0.1778% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(8.22 K = 0.1765% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 32, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(64 = 0.0014% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            51.39 K = 1.1027% Params, 3.36 GMACs = 2.4863% MACs, 6.73 GFLOPS = 2.4773% FLOPs
            (conv2d): Conv2d(51.26 K = 1.0999% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs, 32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.18 GFLOPS = 0.8031% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            16.58 K = 0.3557% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 16.78 MFLOPS = 0.0062% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      855.81 K = 18.3627% Params, 13.96 GMACs = 10.3429% MACs, 28.02 GFLOPS = 10.3183% FLOPs
      (0): InceptionBlock(
        378.75 K = 8.1267% Params, 6.17 GMACs = 4.5747% MACs, 12.39 GFLOPS = 4.5638% FLOPs
        (branch1): ConvBlock(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
          (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          90.69 K = 1.9458% Params, 1.48 GMACs = 1.094% MACs, 2.97 GFLOPS = 1.0919% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          221.76 K = 4.7582% Params, 3.62 GMACs = 2.6852% MACs, 7.26 GFLOPS = 2.6734% FLOPs
          (0): ConvBlock(
            16.58 K = 0.3557% Params, 268.44 MMACs = 0.1989% MACs, 541.07 MFLOPS = 0.1992% FLOPs
            (conv2d): Conv2d(16.45 K = 0.3529% Params, 268.44 MMACs = 0.1989% MACs, 537.92 MFLOPS = 0.1981% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.09 GFLOPS = 0.4% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            33.15 K = 0.7113% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3985% FLOPs
            (conv2d): Conv2d(32.9 K = 0.7058% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        477.06 K = 10.236% Params, 7.78 GMACs = 5.7682% MACs, 15.62 GFLOPS = 5.7514% FLOPs
        (branch1): ConvBlock(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          107.07 K = 2.2974% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2896% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            74.11 K = 1.5902% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8927% FLOPs
            (conv2d): Conv2d(73.86 K = 1.5847% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8904% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          238.14 K = 5.1097% Params, 3.89 GMACs = 2.8841% MACs, 7.8 GFLOPS = 2.8711% FLOPs
          (0): ConvBlock(
            32.96 K = 0.7072% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(32.83 K = 0.7045% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 64, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(128 = 0.0027% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            205.18 K = 4.4025% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4742% FLOPs
            (conv2d): Conv2d(204.93 K = 4.397% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4718% FLOPs, 64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7969% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            65.92 K = 1.4144% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7915% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0.0031% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      3.42 M = 73.2859% Params, 13.96 GMACs = 10.3429% MACs, 27.97 GFLOPS = 10.299% FLOPs
      (0): InceptionBlock(
        1.51 M = 32.4244% Params, 6.17 GMACs = 4.5747% MACs, 12.37 GFLOPS = 4.5553% FLOPs
        (branch1): ConvBlock(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
          (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          361.6 K = 7.7587% Params, 1.48 GMACs = 1.094% MACs, 2.96 GFLOPS = 1.0896% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          885.89 K = 19.0081% Params, 3.62 GMACs = 2.6852% MACs, 7.25 GFLOPS = 2.6711% FLOPs
          (0): ConvBlock(
            65.92 K = 1.4144% Params, 268.44 MMACs = 0.1989% MACs, 538.97 MFLOPS = 0.1985% FLOPs
            (conv2d): Conv2d(65.66 K = 1.4089% Params, 268.44 MMACs = 0.1989% MACs, 537.4 MFLOPS = 0.1979% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3977% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            131.84 K = 2.8288% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3969% FLOPs
            (conv2d): Conv2d(131.33 K = 2.8178% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3958% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (1): InceptionBlock(
        1.9 M = 40.8615% Params, 7.78 GMACs = 5.7682% MACs, 15.59 GFLOPS = 5.7422% FLOPs
        (branch1): ConvBlock(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
          (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
        )
        (branch2): Sequential(
          427.14 K = 9.1649% Params, 1.74 GMACs = 1.2929% MACs, 3.5 GFLOPS = 1.2873% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            295.68 K = 6.3443% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.8911% FLOPs
            (conv2d): Conv2d(295.17 K = 6.3333% Params, 1.21 GMACs = 0.8951% MACs, 2.42 GFLOPS = 0.89% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch3): Sequential(
          951.42 K = 20.4143% Params, 3.89 GMACs = 2.8841% MACs, 7.79 GFLOPS = 2.8688% FLOPs
          (0): ConvBlock(
            131.46 K = 2.8206% Params, 536.87 MMACs = 0.3978% MACs, 1.08 GFLOPS = 0.3961% FLOPs
            (conv2d): Conv2d(131.2 K = 2.8151% Params, 536.87 MMACs = 0.3978% MACs, 1.07 GFLOPS = 0.3956% FLOPs, 1024, 128, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(256 = 0.0055% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0.0002% FLOPs, negative_slope=0.01)
          )
          (1): ConvBlock(
            819.97 K = 17.5937% Params, 3.36 GMACs = 2.4863% MACs, 6.72 GFLOPS = 2.4726% FLOPs
            (conv2d): Conv2d(819.46 K = 17.5827% Params, 3.36 GMACs = 2.4863% MACs, 6.71 GFLOPS = 2.4715% FLOPs, 128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
        (branch4): Sequential(
          262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.16 GFLOPS = 0.7938% FLOPs
          (0): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            262.91 K = 5.6412% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7923% FLOPs
            (conv2d): Conv2d(262.4 K = 5.6302% Params, 1.07 GMACs = 0.7956% MACs, 2.15 GFLOPS = 0.7911% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (norm_layer): BatchNorm2d(512 = 0.011% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0.0008% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act_layer): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, negative_slope=0.01)
          )
        )
      )
      (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0.0015% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (head): Sequential(
    44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 6.68 MFLOPS = 0.0025% FLOPs
    (0): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0.0004% FLOPs, output_size=(1, 1))
    (1): SqueezeDims(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.3, inplace=False)
    (3): Linear(44.08 K = 0.9457% Params, 2.82 MMACs = 0.0021% MACs, 5.64 MFLOPS = 0.0021% FLOPs, in_features=1024, out_features=43, bias=True)
  )
)
---------------------------------------------------------------------------------------------------
Epoch 1/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 1/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 1/30
Train Loss: 3.1516 Train Accuracy: 0.1864
Val Loss: 3.3495 Val Accuracy: 0.1115
Epoch 2/30: 100%|██████████| 42/42 [00:19<00:00,  2.12it/s]
Epoch 2/30: 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]
Epoch 2/30
Train Loss: 2.1779 Train Accuracy: 0.3701
Val Loss: 2.1640 Val Accuracy: 0.3142
Epoch 3/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 3/30: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]
Epoch 3/30
Train Loss: 1.5373 Train Accuracy: 0.5311
Val Loss: 1.9532 Val Accuracy: 0.4088
Epoch 4/30: 100%|██████████| 42/42 [00:19<00:00,  2.13it/s]
Epoch 4/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 4/30
Train Loss: 1.1834 Train Accuracy: 0.6239
Val Loss: 2.0672 Val Accuracy: 0.4054
Epoch 5/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 5/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 5/30
Train Loss: 1.0006 Train Accuracy: 0.6852
Val Loss: 2.1451 Val Accuracy: 0.3412
Epoch 6/30: 100%|██████████| 42/42 [00:19<00:00,  2.13it/s]
Epoch 6/30: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]
Epoch 6/30
Train Loss: 0.8611 Train Accuracy: 0.7280
Val Loss: 2.7987 Val Accuracy: 0.3750
Epoch 7/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 7/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 7/30
Train Loss: 0.8751 Train Accuracy: 0.7205
Val Loss: 2.1615 Val Accuracy: 0.4189
Epoch 8/30: 100%|██████████| 42/42 [00:19<00:00,  2.15it/s]
Epoch 8/30: 100%|██████████| 5/5 [00:01<00:00,  3.02it/s]
Epoch 8/30
Train Loss: 0.6577 Train Accuracy: 0.7811
Val Loss: 2.1424 Val Accuracy: 0.4561
Epoch 9/30: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s]
Epoch 9/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 9/30
Train Loss: 0.6237 Train Accuracy: 0.8106
Val Loss: 2.6184 Val Accuracy: 0.4392
Epoch 10/30: 100%|██████████| 42/42 [00:19<00:00,  2.15it/s]
Epoch 10/30: 100%|██████████| 5/5 [00:01<00:00,  3.23it/s]
Epoch 10/30
Train Loss: 0.5792 Train Accuracy: 0.8144
Val Loss: 1.8939 Val Accuracy: 0.4966
Epoch 11/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 11/30: 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]
Epoch 11/30
Train Loss: 0.3914 Train Accuracy: 0.8761
Val Loss: 1.9034 Val Accuracy: 0.4797
Epoch 12/30: 100%|██████████| 42/42 [00:19<00:00,  2.13it/s]
Epoch 12/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 12/30
Train Loss: 0.3206 Train Accuracy: 0.8951
Val Loss: 2.1367 Val Accuracy: 0.4595
Epoch 13/30: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s]
Epoch 13/30: 100%|██████████| 5/5 [00:01<00:00,  3.01it/s]
Epoch 13/30
Train Loss: 0.2576 Train Accuracy: 0.9182
Val Loss: 2.0331 Val Accuracy: 0.5439
Epoch 14/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 14/30: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]
Epoch 14/30
Train Loss: 0.2321 Train Accuracy: 0.9292
Val Loss: 1.7897 Val Accuracy: 0.5608
Epoch 15/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 15/30: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]
Epoch 15/30
Train Loss: 0.2912 Train Accuracy: 0.9159
Val Loss: 2.1566 Val Accuracy: 0.4662
Epoch 16/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 16/30: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]
Epoch 16/30
Train Loss: 0.2727 Train Accuracy: 0.9235
Val Loss: 1.9278 Val Accuracy: 0.5068
Epoch 17/30: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s]
Epoch 17/30: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]
Epoch 17/30
Train Loss: 0.1281 Train Accuracy: 0.9667
Val Loss: 1.6645 Val Accuracy: 0.5946
Epoch 18/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 18/30: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s]
Epoch 18/30
Train Loss: 0.0807 Train Accuracy: 0.9788
Val Loss: 1.6973 Val Accuracy: 0.6081
Epoch 19/30: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s]
Epoch 19/30: 100%|██████████| 5/5 [00:01<00:00,  3.16it/s]
Epoch 19/30
Train Loss: 0.0642 Train Accuracy: 0.9871
Val Loss: 1.6405 Val Accuracy: 0.5980
Epoch 20/30: 100%|██████████| 42/42 [00:20<00:00,  2.08it/s]
Epoch 20/30: 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]
Epoch 20/30
Train Loss: 0.0428 Train Accuracy: 0.9898
Val Loss: 1.6811 Val Accuracy: 0.6182
Epoch 21/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 21/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 21/30
Train Loss: 0.0428 Train Accuracy: 0.9905
Val Loss: 1.8109 Val Accuracy: 0.6014
Epoch 22/30: 100%|██████████| 42/42 [00:19<00:00,  2.10it/s]
Epoch 22/30: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s]
Epoch 22/30
Train Loss: 0.0381 Train Accuracy: 0.9939
Val Loss: 1.5847 Val Accuracy: 0.6149
Epoch 23/30: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s]
Epoch 23/30: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]
Epoch 23/30
Train Loss: 0.0477 Train Accuracy: 0.9902
Val Loss: 1.6123 Val Accuracy: 0.6250
Epoch 24/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 24/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 24/30
Train Loss: 0.0234 Train Accuracy: 0.9970
Val Loss: 1.6550 Val Accuracy: 0.6284
Epoch 25/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 25/30: 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]
Epoch 25/30
Train Loss: 0.0221 Train Accuracy: 0.9977
Val Loss: 1.7037 Val Accuracy: 0.6250
Epoch 26/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 26/30: 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]
Epoch 26/30
Train Loss: 0.0234 Train Accuracy: 0.9966
Val Loss: 1.7661 Val Accuracy: 0.6216
Epoch 27/30: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s]
Epoch 27/30: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]
Epoch 27/30
Train Loss: 0.0150 Train Accuracy: 0.9989
Val Loss: 1.6794 Val Accuracy: 0.6250
Epoch 28/30: 100%|██████████| 42/42 [00:19<00:00,  2.11it/s]
Epoch 28/30: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s]
Epoch 28/30
Train Loss: 0.0145 Train Accuracy: 0.9977
Val Loss: 1.6732 Val Accuracy: 0.6284
Epoch 29/30: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s]
Epoch 29/30: 100%|██████████| 5/5 [00:01<00:00,  3.20it/s]
Epoch 29/30
Train Loss: 0.0106 Train Accuracy: 0.9992
Val Loss: 1.6810 Val Accuracy: 0.6318
Epoch 30/30: 100%|██████████| 42/42 [00:19<00:00,  2.12it/s]
Epoch 30/30: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]Epoch 30/30
Train Loss: 0.0112 Train Accuracy: 0.9992
Val Loss: 1.7492 Val Accuracy: 0.6182